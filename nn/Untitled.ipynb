{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "f437af1b-18fd-4b7e-ba81-f969a0b23268",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import torch\n",
    "import torchvision ## Contains some utilities for working with the image data\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "662e357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetFromDepthImages(Dataset):\n",
    "    def __init__(self, csv_path, img_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            img_path (string): path to the folder where images are\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        # Transforms\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        # Read the csv file\n",
    "        self.data_info = pd.read_csv(csv_path, header=None)\n",
    "        # First column contains the image paths\n",
    "        self.image_arr = np.asarray(self.data_info.iloc[1:, 0])\n",
    "        # Second column is the labels\n",
    "        self.label_arr = np.asarray(self.data_info.iloc[1:, 1])\n",
    "        # Calculate len\n",
    "        self.data_len = len(self.data_info.index) - 2 \n",
    "\n",
    "        self.img_path = img_path\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image name from the pandas df\n",
    "        single_image_name = self.img_path + '/' + self.image_arr[index]\n",
    "        # Open image\n",
    "        img_as_img = np.load(single_image_name)\n",
    "        img_as_img /= 10000.0\n",
    "        # Transform image to tensor\n",
    "        img_as_tensor = torch.from_numpy(img_as_img.astype('float32'))\n",
    "\n",
    "        # Get label(class) of the image based on the cropped pandas column\n",
    "        single_image_label = self.label_arr[index]\n",
    "\n",
    "        return (img_as_tensor, int(single_image_label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "ebb6d649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9643"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATAPATH = '/home/alex/Documents/datasets/VisualPlanerData/iter_1/out.csv'\n",
    "DATAPATHIMAGES = '/home/alex/Documents/datasets/VisualPlanerData/iter_1'\n",
    "custom_dataset =  CustomDatasetFromDepthImages(DATAPATH, DATAPATHIMAGES)\n",
    "custom_dataset.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "bbb0a6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0273, 0.0129, 1.0000, 0.0243, 0.0241, 0.0248, 0.0264, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0406, 0.0409, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.0434, 0.0262, 0.0255, 0.0261, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 0.0124, 0.0125, 0.0125, 0.0129, 0.0247, 0.0246, 0.0637,\n",
      "        0.0420, 0.0378, 0.0388, 0.0570, 0.0606, 0.0224]), 2)\n"
     ]
    }
   ],
   "source": [
    "print(custom_dataset[0])\n",
    "# plt.imshow(image, cmap = 'gray')\n",
    "# print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "69cc6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST dataset(images and labels)\n",
    "# mnist_dataset = MNIST(root = 'data/', train = True, transform = transforms.ToTensor())\n",
    "# print(mnist_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "741a78ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42]) 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_tensor, label = custom_dataset[0]\n",
    "print(image_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "fa8ca06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0273, 0.0129, 1.0000, 0.0243, 0.0241, 0.0248, 0.0264, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0406, 0.0409, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.0434, 0.0262, 0.0255, 0.0261, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 0.0124, 0.0125, 0.0125, 0.0129, 0.0247, 0.0246, 0.0637,\n",
      "        0.0420, 0.0378, 0.0388, 0.0570, 0.0606, 0.0224])\n",
      "tensor(1.) tensor(0.0124)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(image_tensor)\n",
    "print(torch.max(image_tensor), torch.min(image_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "eb91f2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of Train Datasets:  7714\n",
      "length of Validation Datasets:  1929\n"
     ]
    }
   ],
   "source": [
    "k = 0.8\n",
    "train_data_len = round(custom_dataset.data_len * k)\n",
    "validation_data_len = round(custom_dataset.data_len * (1-k))\n",
    "train_data, validation_data = random_split(custom_dataset, [train_data_len, validation_data_len])\n",
    "## Print the length of train and validation datasets\n",
    "print(\"length of Train Datasets: \", len(train_data))\n",
    "print(\"length of Validation Datasets: \", len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "ae595554",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size, shuffle = True)\n",
    "val_loader = DataLoader(validation_data, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "82dd8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim = 1)\n",
    "    return(torch.tensor(torch.sum(preds == labels).item()/ len(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "3a56e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 42\n",
    "num_classes = 6\n",
    "\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, 42)\n",
    "        self.linear2 = nn.Linear(42, 30)\n",
    "        self.linear3 = nn.Linear(30, 15)\n",
    "        self.linear4 = nn.Linear(15, num_classes)\n",
    "\n",
    "        self.act = nn.SELU()\n",
    "    \n",
    "    def forward(self, xb):\n",
    "\n",
    "        out = self.linear1(xb)\n",
    "        out = self.act(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear4(out)\n",
    "        return(out)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images) ## Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) ## Calculate the loss\n",
    "        return(loss)\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return({'val_loss':loss, 'val_acc': acc})\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return({'val_loss': epoch_loss.item(), 'val_acc' : epoch_acc.item()})\n",
    "    \n",
    "    def epoch_end(self, epoch,result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "        \n",
    "    \n",
    "model = MnistModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "62ac7ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(type(train_data))\n",
    "# custom_dataset[0][0]\n",
    "model.forward(custom_dataset[214][0]).size()\n",
    "# custom_dataset[0][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "017dbb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return(model.validation_epoch_end(outputs))\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.Adam):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        ## Training Phas\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        ## Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "1c772649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 1.7088547945022583, 'val_acc': 0.1810980886220932}"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "b2cdd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.2391, val_acc: 0.6175\n",
      "Epoch [1], val_loss: 1.2037, val_acc: 0.6326\n",
      "Epoch [2], val_loss: 1.1525, val_acc: 0.6253\n",
      "Epoch [3], val_loss: 1.1347, val_acc: 0.6311\n",
      "Epoch [4], val_loss: 1.1150, val_acc: 0.6463\n",
      "Epoch [5], val_loss: 1.0874, val_acc: 0.6458\n",
      "Epoch [6], val_loss: 1.0712, val_acc: 0.6497\n",
      "Epoch [7], val_loss: 1.0734, val_acc: 0.6565\n",
      "Epoch [8], val_loss: 1.0392, val_acc: 0.6565\n",
      "Epoch [9], val_loss: 1.0162, val_acc: 0.6629\n",
      "Epoch [10], val_loss: 0.9988, val_acc: 0.6682\n",
      "Epoch [11], val_loss: 0.9661, val_acc: 0.6736\n",
      "Epoch [12], val_loss: 0.9403, val_acc: 0.6780\n",
      "Epoch [13], val_loss: 0.9218, val_acc: 0.6844\n",
      "Epoch [14], val_loss: 0.9153, val_acc: 0.6951\n",
      "Epoch [15], val_loss: 0.8903, val_acc: 0.6941\n",
      "Epoch [16], val_loss: 0.8687, val_acc: 0.7010\n",
      "Epoch [17], val_loss: 0.8700, val_acc: 0.7005\n",
      "Epoch [18], val_loss: 0.8573, val_acc: 0.7102\n",
      "Epoch [19], val_loss: 0.8934, val_acc: 0.6990\n",
      "Epoch [20], val_loss: 0.8393, val_acc: 0.7185\n",
      "Epoch [21], val_loss: 0.8517, val_acc: 0.7014\n",
      "Epoch [22], val_loss: 0.8399, val_acc: 0.7107\n",
      "Epoch [23], val_loss: 0.8239, val_acc: 0.7137\n",
      "Epoch [24], val_loss: 0.8204, val_acc: 0.7151\n",
      "Epoch [25], val_loss: 0.8191, val_acc: 0.7166\n",
      "Epoch [26], val_loss: 0.8137, val_acc: 0.7220\n",
      "Epoch [27], val_loss: 0.8282, val_acc: 0.7201\n",
      "Epoch [28], val_loss: 0.8067, val_acc: 0.7200\n",
      "Epoch [29], val_loss: 0.8035, val_acc: 0.7240\n",
      "Epoch [30], val_loss: 0.7988, val_acc: 0.7268\n",
      "Epoch [31], val_loss: 0.8171, val_acc: 0.7195\n",
      "Epoch [32], val_loss: 0.7867, val_acc: 0.7293\n",
      "Epoch [33], val_loss: 0.7920, val_acc: 0.7244\n",
      "Epoch [34], val_loss: 0.7905, val_acc: 0.7262\n",
      "Epoch [35], val_loss: 0.7901, val_acc: 0.7303\n",
      "Epoch [36], val_loss: 0.7729, val_acc: 0.7307\n",
      "Epoch [37], val_loss: 0.7681, val_acc: 0.7406\n",
      "Epoch [38], val_loss: 0.7653, val_acc: 0.7328\n",
      "Epoch [39], val_loss: 0.7722, val_acc: 0.7288\n",
      "Epoch [40], val_loss: 0.7615, val_acc: 0.7342\n",
      "Epoch [41], val_loss: 0.7469, val_acc: 0.7395\n",
      "Epoch [42], val_loss: 0.7465, val_acc: 0.7410\n",
      "Epoch [43], val_loss: 0.7517, val_acc: 0.7366\n",
      "Epoch [44], val_loss: 0.7449, val_acc: 0.7406\n",
      "Epoch [45], val_loss: 0.7358, val_acc: 0.7587\n",
      "Epoch [46], val_loss: 0.7413, val_acc: 0.7444\n",
      "Epoch [47], val_loss: 0.7364, val_acc: 0.7440\n",
      "Epoch [48], val_loss: 0.7257, val_acc: 0.7384\n",
      "Epoch [49], val_loss: 0.7434, val_acc: 0.7332\n",
      "Epoch [50], val_loss: 0.7298, val_acc: 0.7494\n",
      "Epoch [51], val_loss: 0.7160, val_acc: 0.7556\n",
      "Epoch [52], val_loss: 0.7073, val_acc: 0.7469\n",
      "Epoch [53], val_loss: 0.7148, val_acc: 0.7597\n",
      "Epoch [54], val_loss: 0.7127, val_acc: 0.7488\n",
      "Epoch [55], val_loss: 0.7194, val_acc: 0.7420\n",
      "Epoch [56], val_loss: 0.7066, val_acc: 0.7660\n",
      "Epoch [57], val_loss: 0.7075, val_acc: 0.7498\n",
      "Epoch [58], val_loss: 0.7091, val_acc: 0.7567\n",
      "Epoch [59], val_loss: 0.7199, val_acc: 0.7405\n",
      "Epoch [60], val_loss: 0.7104, val_acc: 0.7512\n",
      "Epoch [61], val_loss: 0.6941, val_acc: 0.7665\n",
      "Epoch [62], val_loss: 0.7453, val_acc: 0.7381\n",
      "Epoch [63], val_loss: 0.7037, val_acc: 0.7493\n",
      "Epoch [64], val_loss: 0.6936, val_acc: 0.7675\n",
      "Epoch [65], val_loss: 0.7202, val_acc: 0.7439\n",
      "Epoch [66], val_loss: 0.7012, val_acc: 0.7483\n",
      "Epoch [67], val_loss: 0.6849, val_acc: 0.7615\n",
      "Epoch [68], val_loss: 0.6911, val_acc: 0.7532\n",
      "Epoch [69], val_loss: 0.6899, val_acc: 0.7626\n",
      "Epoch [70], val_loss: 0.6849, val_acc: 0.7665\n",
      "Epoch [71], val_loss: 0.6967, val_acc: 0.7503\n",
      "Epoch [72], val_loss: 0.6862, val_acc: 0.7606\n",
      "Epoch [73], val_loss: 0.6825, val_acc: 0.7670\n",
      "Epoch [74], val_loss: 0.6848, val_acc: 0.7670\n",
      "Epoch [75], val_loss: 0.6969, val_acc: 0.7655\n",
      "Epoch [76], val_loss: 0.6732, val_acc: 0.7680\n",
      "Epoch [77], val_loss: 0.6782, val_acc: 0.7680\n",
      "Epoch [78], val_loss: 0.6899, val_acc: 0.7615\n",
      "Epoch [79], val_loss: 0.6816, val_acc: 0.7728\n",
      "Epoch [80], val_loss: 0.6864, val_acc: 0.7650\n",
      "Epoch [81], val_loss: 0.6960, val_acc: 0.7650\n",
      "Epoch [82], val_loss: 0.6747, val_acc: 0.7743\n",
      "Epoch [83], val_loss: 0.6857, val_acc: 0.7670\n",
      "Epoch [84], val_loss: 0.6791, val_acc: 0.7743\n",
      "Epoch [85], val_loss: 0.6716, val_acc: 0.7724\n",
      "Epoch [86], val_loss: 0.6758, val_acc: 0.7694\n",
      "Epoch [87], val_loss: 0.6831, val_acc: 0.7635\n",
      "Epoch [88], val_loss: 0.6822, val_acc: 0.7680\n",
      "Epoch [89], val_loss: 0.6715, val_acc: 0.7704\n",
      "Epoch [90], val_loss: 0.6946, val_acc: 0.7704\n",
      "Epoch [91], val_loss: 0.6684, val_acc: 0.7807\n",
      "Epoch [92], val_loss: 0.6688, val_acc: 0.7748\n",
      "Epoch [93], val_loss: 0.6869, val_acc: 0.7665\n",
      "Epoch [94], val_loss: 0.6653, val_acc: 0.7782\n",
      "Epoch [95], val_loss: 0.6799, val_acc: 0.7802\n",
      "Epoch [96], val_loss: 0.6654, val_acc: 0.7792\n",
      "Epoch [97], val_loss: 0.6873, val_acc: 0.7689\n",
      "Epoch [98], val_loss: 0.6655, val_acc: 0.7753\n",
      "Epoch [99], val_loss: 0.6786, val_acc: 0.7724\n",
      "Epoch [100], val_loss: 0.6659, val_acc: 0.7816\n",
      "Epoch [101], val_loss: 0.6727, val_acc: 0.7708\n",
      "Epoch [102], val_loss: 0.6657, val_acc: 0.7772\n",
      "Epoch [103], val_loss: 0.6598, val_acc: 0.7846\n",
      "Epoch [104], val_loss: 0.6722, val_acc: 0.7767\n",
      "Epoch [105], val_loss: 0.6855, val_acc: 0.7714\n",
      "Epoch [106], val_loss: 0.6659, val_acc: 0.7841\n",
      "Epoch [107], val_loss: 0.6571, val_acc: 0.7821\n",
      "Epoch [108], val_loss: 0.6745, val_acc: 0.7724\n",
      "Epoch [109], val_loss: 0.6640, val_acc: 0.7831\n",
      "Epoch [110], val_loss: 0.6651, val_acc: 0.7826\n",
      "Epoch [111], val_loss: 0.6569, val_acc: 0.7860\n",
      "Epoch [112], val_loss: 0.6686, val_acc: 0.7807\n",
      "Epoch [113], val_loss: 0.6897, val_acc: 0.7748\n",
      "Epoch [114], val_loss: 0.6816, val_acc: 0.7699\n",
      "Epoch [115], val_loss: 0.6858, val_acc: 0.7714\n",
      "Epoch [116], val_loss: 0.6672, val_acc: 0.7865\n",
      "Epoch [117], val_loss: 0.6585, val_acc: 0.7816\n",
      "Epoch [118], val_loss: 0.6591, val_acc: 0.7836\n",
      "Epoch [119], val_loss: 0.6740, val_acc: 0.7826\n",
      "Epoch [120], val_loss: 0.6517, val_acc: 0.7870\n",
      "Epoch [121], val_loss: 0.6647, val_acc: 0.7875\n",
      "Epoch [122], val_loss: 0.6778, val_acc: 0.7777\n",
      "Epoch [123], val_loss: 0.6804, val_acc: 0.7728\n",
      "Epoch [124], val_loss: 0.6944, val_acc: 0.7807\n",
      "Epoch [125], val_loss: 0.6592, val_acc: 0.7899\n",
      "Epoch [126], val_loss: 0.6859, val_acc: 0.7826\n",
      "Epoch [127], val_loss: 0.6665, val_acc: 0.7841\n",
      "Epoch [128], val_loss: 0.6790, val_acc: 0.7758\n",
      "Epoch [129], val_loss: 0.6555, val_acc: 0.7885\n",
      "Epoch [130], val_loss: 0.6529, val_acc: 0.7894\n",
      "Epoch [131], val_loss: 0.6541, val_acc: 0.7885\n",
      "Epoch [132], val_loss: 0.6679, val_acc: 0.7787\n",
      "Epoch [133], val_loss: 0.6505, val_acc: 0.7899\n",
      "Epoch [134], val_loss: 0.6530, val_acc: 0.7880\n",
      "Epoch [135], val_loss: 0.6567, val_acc: 0.7846\n",
      "Epoch [136], val_loss: 0.6626, val_acc: 0.7821\n",
      "Epoch [137], val_loss: 0.6568, val_acc: 0.7904\n",
      "Epoch [138], val_loss: 0.6454, val_acc: 0.7914\n",
      "Epoch [139], val_loss: 0.6896, val_acc: 0.7792\n",
      "Epoch [140], val_loss: 0.6458, val_acc: 0.7977\n",
      "Epoch [141], val_loss: 0.6529, val_acc: 0.7919\n",
      "Epoch [142], val_loss: 0.6549, val_acc: 0.7943\n",
      "Epoch [143], val_loss: 0.6593, val_acc: 0.7890\n",
      "Epoch [144], val_loss: 0.6471, val_acc: 0.7870\n",
      "Epoch [145], val_loss: 0.6552, val_acc: 0.7943\n",
      "Epoch [146], val_loss: 0.6497, val_acc: 0.7919\n",
      "Epoch [147], val_loss: 0.6575, val_acc: 0.7914\n",
      "Epoch [148], val_loss: 0.6648, val_acc: 0.7909\n",
      "Epoch [149], val_loss: 0.6471, val_acc: 0.7977\n",
      "Epoch [150], val_loss: 0.6718, val_acc: 0.7885\n",
      "Epoch [151], val_loss: 0.6591, val_acc: 0.7855\n",
      "Epoch [152], val_loss: 0.6743, val_acc: 0.7792\n",
      "Epoch [153], val_loss: 0.6503, val_acc: 0.7904\n",
      "Epoch [154], val_loss: 0.6621, val_acc: 0.7943\n",
      "Epoch [155], val_loss: 0.6827, val_acc: 0.7816\n",
      "Epoch [156], val_loss: 0.6762, val_acc: 0.7870\n",
      "Epoch [157], val_loss: 0.6634, val_acc: 0.7919\n",
      "Epoch [158], val_loss: 0.7020, val_acc: 0.7743\n",
      "Epoch [159], val_loss: 0.6504, val_acc: 0.7953\n",
      "Epoch [160], val_loss: 0.6433, val_acc: 0.7982\n",
      "Epoch [161], val_loss: 0.6515, val_acc: 0.7904\n",
      "Epoch [162], val_loss: 0.6683, val_acc: 0.7846\n",
      "Epoch [163], val_loss: 0.6856, val_acc: 0.7688\n",
      "Epoch [164], val_loss: 0.6844, val_acc: 0.7865\n",
      "Epoch [165], val_loss: 0.6427, val_acc: 0.7982\n",
      "Epoch [166], val_loss: 0.6490, val_acc: 0.7924\n",
      "Epoch [167], val_loss: 0.6521, val_acc: 0.8002\n",
      "Epoch [168], val_loss: 0.6486, val_acc: 0.8007\n",
      "Epoch [169], val_loss: 0.6472, val_acc: 0.7953\n",
      "Epoch [170], val_loss: 0.6516, val_acc: 0.7973\n",
      "Epoch [171], val_loss: 0.6647, val_acc: 0.7963\n",
      "Epoch [172], val_loss: 0.6630, val_acc: 0.7933\n",
      "Epoch [173], val_loss: 0.6679, val_acc: 0.7933\n",
      "Epoch [174], val_loss: 0.6824, val_acc: 0.7826\n",
      "Epoch [175], val_loss: 0.6610, val_acc: 0.7953\n",
      "Epoch [176], val_loss: 0.6510, val_acc: 0.7992\n",
      "Epoch [177], val_loss: 0.6487, val_acc: 0.8021\n",
      "Epoch [178], val_loss: 0.6957, val_acc: 0.7816\n",
      "Epoch [179], val_loss: 0.6427, val_acc: 0.7958\n",
      "Epoch [180], val_loss: 0.6536, val_acc: 0.7973\n",
      "Epoch [181], val_loss: 0.6807, val_acc: 0.7929\n",
      "Epoch [182], val_loss: 0.6431, val_acc: 0.8016\n",
      "Epoch [183], val_loss: 0.7250, val_acc: 0.7777\n",
      "Epoch [184], val_loss: 0.6910, val_acc: 0.7742\n",
      "Epoch [185], val_loss: 0.6580, val_acc: 0.7909\n",
      "Epoch [186], val_loss: 0.6517, val_acc: 0.7963\n",
      "Epoch [187], val_loss: 0.6473, val_acc: 0.7997\n",
      "Epoch [188], val_loss: 0.6552, val_acc: 0.8036\n",
      "Epoch [189], val_loss: 0.6558, val_acc: 0.7943\n",
      "Epoch [190], val_loss: 0.6491, val_acc: 0.7968\n",
      "Epoch [191], val_loss: 0.6708, val_acc: 0.7929\n",
      "Epoch [192], val_loss: 0.6592, val_acc: 0.8002\n",
      "Epoch [193], val_loss: 0.6483, val_acc: 0.7987\n",
      "Epoch [194], val_loss: 0.6504, val_acc: 0.8021\n",
      "Epoch [195], val_loss: 0.6651, val_acc: 0.7932\n",
      "Epoch [196], val_loss: 0.6589, val_acc: 0.7948\n",
      "Epoch [197], val_loss: 0.6578, val_acc: 0.7977\n",
      "Epoch [198], val_loss: 0.6672, val_acc: 0.7894\n",
      "Epoch [199], val_loss: 0.6573, val_acc: 0.7987\n",
      "Epoch [200], val_loss: 0.6649, val_acc: 0.7973\n",
      "Epoch [201], val_loss: 0.6769, val_acc: 0.7904\n",
      "Epoch [202], val_loss: 0.7214, val_acc: 0.7689\n",
      "Epoch [203], val_loss: 0.6461, val_acc: 0.7982\n",
      "Epoch [204], val_loss: 0.6773, val_acc: 0.7810\n",
      "Epoch [205], val_loss: 0.6620, val_acc: 0.7987\n",
      "Epoch [206], val_loss: 0.6864, val_acc: 0.7929\n",
      "Epoch [207], val_loss: 0.6564, val_acc: 0.8051\n",
      "Epoch [208], val_loss: 0.6708, val_acc: 0.7974\n",
      "Epoch [209], val_loss: 0.6541, val_acc: 0.8052\n",
      "Epoch [210], val_loss: 0.6957, val_acc: 0.7731\n",
      "Epoch [211], val_loss: 0.6620, val_acc: 0.7968\n",
      "Epoch [212], val_loss: 0.6741, val_acc: 0.8007\n",
      "Epoch [213], val_loss: 0.6667, val_acc: 0.7947\n",
      "Epoch [214], val_loss: 0.6658, val_acc: 0.7943\n",
      "Epoch [215], val_loss: 0.6676, val_acc: 0.7938\n",
      "Epoch [216], val_loss: 0.6458, val_acc: 0.8065\n",
      "Epoch [217], val_loss: 0.6538, val_acc: 0.8046\n",
      "Epoch [218], val_loss: 0.6740, val_acc: 0.7968\n",
      "Epoch [219], val_loss: 0.6453, val_acc: 0.8125\n",
      "Epoch [220], val_loss: 0.6565, val_acc: 0.8076\n",
      "Epoch [221], val_loss: 0.6624, val_acc: 0.8021\n",
      "Epoch [222], val_loss: 0.6727, val_acc: 0.7993\n",
      "Epoch [223], val_loss: 0.6545, val_acc: 0.8041\n",
      "Epoch [224], val_loss: 0.6506, val_acc: 0.8036\n",
      "Epoch [225], val_loss: 0.6776, val_acc: 0.7825\n",
      "Epoch [226], val_loss: 0.6760, val_acc: 0.7899\n",
      "Epoch [227], val_loss: 0.6555, val_acc: 0.8110\n",
      "Epoch [228], val_loss: 0.6425, val_acc: 0.8135\n",
      "Epoch [229], val_loss: 0.6684, val_acc: 0.7977\n",
      "Epoch [230], val_loss: 0.6795, val_acc: 0.7998\n",
      "Epoch [231], val_loss: 0.6964, val_acc: 0.7855\n",
      "Epoch [232], val_loss: 0.6634, val_acc: 0.8041\n",
      "Epoch [233], val_loss: 0.6729, val_acc: 0.7908\n",
      "Epoch [234], val_loss: 0.6551, val_acc: 0.8105\n",
      "Epoch [235], val_loss: 0.6896, val_acc: 0.7987\n",
      "Epoch [236], val_loss: 0.7039, val_acc: 0.7792\n",
      "Epoch [237], val_loss: 0.6823, val_acc: 0.7898\n",
      "Epoch [238], val_loss: 0.6866, val_acc: 0.7933\n",
      "Epoch [239], val_loss: 0.6553, val_acc: 0.8007\n",
      "Epoch [240], val_loss: 0.6540, val_acc: 0.8086\n",
      "Epoch [241], val_loss: 0.7172, val_acc: 0.7924\n",
      "Epoch [242], val_loss: 0.6533, val_acc: 0.8140\n",
      "Epoch [243], val_loss: 0.6748, val_acc: 0.8057\n",
      "Epoch [244], val_loss: 0.6640, val_acc: 0.7992\n",
      "Epoch [245], val_loss: 0.7119, val_acc: 0.7803\n",
      "Epoch [246], val_loss: 0.6664, val_acc: 0.8027\n",
      "Epoch [247], val_loss: 0.6555, val_acc: 0.8076\n",
      "Epoch [248], val_loss: 0.6739, val_acc: 0.7977\n",
      "Epoch [249], val_loss: 0.6730, val_acc: 0.7968\n",
      "Epoch [250], val_loss: 0.6460, val_acc: 0.8086\n",
      "Epoch [251], val_loss: 0.6689, val_acc: 0.8008\n",
      "Epoch [252], val_loss: 0.6779, val_acc: 0.8022\n",
      "Epoch [253], val_loss: 0.6926, val_acc: 0.7963\n",
      "Epoch [254], val_loss: 0.6795, val_acc: 0.8027\n",
      "Epoch [255], val_loss: 0.6540, val_acc: 0.8110\n",
      "Epoch [256], val_loss: 0.6728, val_acc: 0.8012\n",
      "Epoch [257], val_loss: 0.6722, val_acc: 0.7958\n",
      "Epoch [258], val_loss: 0.6866, val_acc: 0.8013\n",
      "Epoch [259], val_loss: 0.6678, val_acc: 0.8070\n",
      "Epoch [260], val_loss: 0.6633, val_acc: 0.8060\n",
      "Epoch [261], val_loss: 0.6633, val_acc: 0.8026\n",
      "Epoch [262], val_loss: 0.6669, val_acc: 0.8037\n",
      "Epoch [263], val_loss: 0.6505, val_acc: 0.8095\n",
      "Epoch [264], val_loss: 0.6712, val_acc: 0.8012\n",
      "Epoch [265], val_loss: 0.6513, val_acc: 0.8071\n",
      "Epoch [266], val_loss: 0.6773, val_acc: 0.7963\n",
      "Epoch [267], val_loss: 0.7009, val_acc: 0.7997\n",
      "Epoch [268], val_loss: 0.6553, val_acc: 0.8070\n",
      "Epoch [269], val_loss: 0.6656, val_acc: 0.8021\n",
      "Epoch [270], val_loss: 0.6763, val_acc: 0.8081\n",
      "Epoch [271], val_loss: 0.6744, val_acc: 0.8031\n",
      "Epoch [272], val_loss: 0.6845, val_acc: 0.8016\n",
      "Epoch [273], val_loss: 0.6664, val_acc: 0.8096\n",
      "Epoch [274], val_loss: 0.6808, val_acc: 0.7948\n",
      "Epoch [275], val_loss: 0.7211, val_acc: 0.7865\n",
      "Epoch [276], val_loss: 0.6786, val_acc: 0.7948\n",
      "Epoch [277], val_loss: 0.6904, val_acc: 0.8026\n",
      "Epoch [278], val_loss: 0.6909, val_acc: 0.7998\n",
      "Epoch [279], val_loss: 0.6646, val_acc: 0.7963\n",
      "Epoch [280], val_loss: 0.8663, val_acc: 0.7101\n",
      "Epoch [281], val_loss: 0.6684, val_acc: 0.7977\n",
      "Epoch [282], val_loss: 0.6753, val_acc: 0.8036\n",
      "Epoch [283], val_loss: 0.7271, val_acc: 0.7649\n",
      "Epoch [284], val_loss: 0.6888, val_acc: 0.7973\n",
      "Epoch [285], val_loss: 0.6853, val_acc: 0.7875\n",
      "Epoch [286], val_loss: 0.6795, val_acc: 0.8051\n",
      "Epoch [287], val_loss: 0.6841, val_acc: 0.7938\n",
      "Epoch [288], val_loss: 0.6703, val_acc: 0.8052\n",
      "Epoch [289], val_loss: 0.6874, val_acc: 0.8007\n",
      "Epoch [290], val_loss: 0.7104, val_acc: 0.8002\n",
      "Epoch [291], val_loss: 0.6769, val_acc: 0.7987\n",
      "Epoch [292], val_loss: 0.6812, val_acc: 0.8026\n",
      "Epoch [293], val_loss: 0.6762, val_acc: 0.8110\n",
      "Epoch [294], val_loss: 0.6697, val_acc: 0.8052\n",
      "Epoch [295], val_loss: 0.7017, val_acc: 0.7881\n",
      "Epoch [296], val_loss: 0.6736, val_acc: 0.8051\n",
      "Epoch [297], val_loss: 0.6756, val_acc: 0.7933\n",
      "Epoch [298], val_loss: 0.6811, val_acc: 0.8016\n",
      "Epoch [299], val_loss: 0.6708, val_acc: 0.7992\n",
      "Epoch [300], val_loss: 0.6783, val_acc: 0.8046\n",
      "Epoch [301], val_loss: 0.6739, val_acc: 0.8002\n",
      "Epoch [302], val_loss: 0.6814, val_acc: 0.7929\n",
      "Epoch [303], val_loss: 0.6706, val_acc: 0.8021\n",
      "Epoch [304], val_loss: 0.6762, val_acc: 0.8110\n",
      "Epoch [305], val_loss: 0.6695, val_acc: 0.8080\n",
      "Epoch [306], val_loss: 0.6946, val_acc: 0.7977\n",
      "Epoch [307], val_loss: 0.6918, val_acc: 0.8066\n",
      "Epoch [308], val_loss: 0.6960, val_acc: 0.8060\n",
      "Epoch [309], val_loss: 0.6726, val_acc: 0.7992\n",
      "Epoch [310], val_loss: 0.6852, val_acc: 0.8046\n",
      "Epoch [311], val_loss: 0.6887, val_acc: 0.7973\n",
      "Epoch [312], val_loss: 0.7077, val_acc: 0.7982\n",
      "Epoch [313], val_loss: 0.6833, val_acc: 0.8031\n",
      "Epoch [314], val_loss: 0.7319, val_acc: 0.7899\n",
      "Epoch [315], val_loss: 0.6708, val_acc: 0.8076\n",
      "Epoch [316], val_loss: 0.6968, val_acc: 0.7968\n",
      "Epoch [317], val_loss: 0.6929, val_acc: 0.7953\n",
      "Epoch [318], val_loss: 0.7118, val_acc: 0.7919\n",
      "Epoch [319], val_loss: 0.6821, val_acc: 0.8047\n",
      "Epoch [320], val_loss: 0.6853, val_acc: 0.7982\n",
      "Epoch [321], val_loss: 0.6938, val_acc: 0.8105\n",
      "Epoch [322], val_loss: 0.7301, val_acc: 0.7703\n",
      "Epoch [323], val_loss: 0.6814, val_acc: 0.7963\n",
      "Epoch [324], val_loss: 0.6981, val_acc: 0.8016\n",
      "Epoch [325], val_loss: 0.7035, val_acc: 0.7969\n",
      "Epoch [326], val_loss: 0.7020, val_acc: 0.8086\n",
      "Epoch [327], val_loss: 0.6838, val_acc: 0.8031\n",
      "Epoch [328], val_loss: 0.7165, val_acc: 0.8013\n",
      "Epoch [329], val_loss: 0.6780, val_acc: 0.8031\n",
      "Epoch [330], val_loss: 0.7622, val_acc: 0.7680\n",
      "Epoch [331], val_loss: 0.7027, val_acc: 0.8012\n",
      "Epoch [332], val_loss: 0.7291, val_acc: 0.8037\n",
      "Epoch [333], val_loss: 0.7019, val_acc: 0.7997\n",
      "Epoch [334], val_loss: 0.7003, val_acc: 0.8031\n",
      "Epoch [335], val_loss: 0.7032, val_acc: 0.7890\n",
      "Epoch [336], val_loss: 0.7273, val_acc: 0.7805\n",
      "Epoch [337], val_loss: 0.6885, val_acc: 0.7924\n",
      "Epoch [338], val_loss: 0.6852, val_acc: 0.8012\n",
      "Epoch [339], val_loss: 0.6856, val_acc: 0.8046\n",
      "Epoch [340], val_loss: 0.7051, val_acc: 0.8031\n",
      "Epoch [341], val_loss: 0.7079, val_acc: 0.8002\n",
      "Epoch [342], val_loss: 0.6966, val_acc: 0.8071\n",
      "Epoch [343], val_loss: 0.7474, val_acc: 0.7644\n",
      "Epoch [344], val_loss: 0.7378, val_acc: 0.7890\n",
      "Epoch [345], val_loss: 0.7061, val_acc: 0.8115\n",
      "Epoch [346], val_loss: 0.6973, val_acc: 0.7963\n",
      "Epoch [347], val_loss: 0.7038, val_acc: 0.8031\n",
      "Epoch [348], val_loss: 0.7167, val_acc: 0.7938\n",
      "Epoch [349], val_loss: 0.7022, val_acc: 0.7992\n",
      "Epoch [350], val_loss: 0.6898, val_acc: 0.8051\n",
      "Epoch [351], val_loss: 0.6817, val_acc: 0.8105\n",
      "Epoch [352], val_loss: 0.7258, val_acc: 0.7968\n",
      "Epoch [353], val_loss: 0.7175, val_acc: 0.7898\n",
      "Epoch [354], val_loss: 0.6889, val_acc: 0.8070\n",
      "Epoch [355], val_loss: 0.7116, val_acc: 0.8056\n",
      "Epoch [356], val_loss: 0.7009, val_acc: 0.8052\n",
      "Epoch [357], val_loss: 0.7101, val_acc: 0.7979\n",
      "Epoch [358], val_loss: 0.7147, val_acc: 0.8031\n",
      "Epoch [359], val_loss: 0.7020, val_acc: 0.8036\n",
      "Epoch [360], val_loss: 0.7240, val_acc: 0.7977\n",
      "Epoch [361], val_loss: 0.7092, val_acc: 0.7987\n",
      "Epoch [362], val_loss: 0.7068, val_acc: 0.8026\n",
      "Epoch [363], val_loss: 0.7250, val_acc: 0.8080\n",
      "Epoch [364], val_loss: 0.7324, val_acc: 0.7938\n",
      "Epoch [365], val_loss: 0.7110, val_acc: 0.8036\n",
      "Epoch [366], val_loss: 0.7283, val_acc: 0.8052\n",
      "Epoch [367], val_loss: 0.7209, val_acc: 0.7924\n",
      "Epoch [368], val_loss: 0.7082, val_acc: 0.8026\n",
      "Epoch [369], val_loss: 0.7222, val_acc: 0.8060\n",
      "Epoch [370], val_loss: 0.7158, val_acc: 0.7997\n",
      "Epoch [371], val_loss: 0.7302, val_acc: 0.7992\n",
      "Epoch [372], val_loss: 0.7172, val_acc: 0.8110\n",
      "Epoch [373], val_loss: 0.7227, val_acc: 0.7948\n",
      "Epoch [374], val_loss: 0.7116, val_acc: 0.8041\n",
      "Epoch [375], val_loss: 0.7570, val_acc: 0.7885\n",
      "Epoch [376], val_loss: 0.7042, val_acc: 0.7979\n",
      "Epoch [377], val_loss: 0.7285, val_acc: 0.7919\n",
      "Epoch [378], val_loss: 0.7141, val_acc: 0.8031\n",
      "Epoch [379], val_loss: 0.7346, val_acc: 0.8016\n",
      "Epoch [380], val_loss: 0.7127, val_acc: 0.8042\n",
      "Epoch [381], val_loss: 0.7134, val_acc: 0.7904\n",
      "Epoch [382], val_loss: 0.7396, val_acc: 0.7963\n",
      "Epoch [383], val_loss: 0.7413, val_acc: 0.7807\n",
      "Epoch [384], val_loss: 0.7116, val_acc: 0.7987\n",
      "Epoch [385], val_loss: 0.7025, val_acc: 0.8149\n",
      "Epoch [386], val_loss: 0.7486, val_acc: 0.7830\n",
      "Epoch [387], val_loss: 0.7071, val_acc: 0.8016\n",
      "Epoch [388], val_loss: 0.7226, val_acc: 0.7944\n",
      "Epoch [389], val_loss: 0.7279, val_acc: 0.8012\n",
      "Epoch [390], val_loss: 0.7141, val_acc: 0.8109\n",
      "Epoch [391], val_loss: 0.7210, val_acc: 0.8046\n",
      "Epoch [392], val_loss: 0.7044, val_acc: 0.8125\n",
      "Epoch [393], val_loss: 0.7053, val_acc: 0.8021\n",
      "Epoch [394], val_loss: 0.7194, val_acc: 0.7992\n",
      "Epoch [395], val_loss: 0.7576, val_acc: 0.7763\n",
      "Epoch [396], val_loss: 0.7262, val_acc: 0.8041\n",
      "Epoch [397], val_loss: 0.7586, val_acc: 0.7904\n",
      "Epoch [398], val_loss: 0.7325, val_acc: 0.8036\n",
      "Epoch [399], val_loss: 0.7307, val_acc: 0.8046\n",
      "Epoch [400], val_loss: 0.7204, val_acc: 0.8002\n",
      "Epoch [401], val_loss: 0.7414, val_acc: 0.7973\n",
      "Epoch [402], val_loss: 0.7380, val_acc: 0.8042\n",
      "Epoch [403], val_loss: 0.7305, val_acc: 0.8056\n",
      "Epoch [404], val_loss: 0.7492, val_acc: 0.7982\n",
      "Epoch [405], val_loss: 0.7425, val_acc: 0.8036\n",
      "Epoch [406], val_loss: 0.7400, val_acc: 0.7894\n",
      "Epoch [407], val_loss: 0.7531, val_acc: 0.7875\n",
      "Epoch [408], val_loss: 0.7239, val_acc: 0.8104\n",
      "Epoch [409], val_loss: 0.7443, val_acc: 0.8027\n",
      "Epoch [410], val_loss: 0.7826, val_acc: 0.7811\n",
      "Epoch [411], val_loss: 0.7265, val_acc: 0.8096\n",
      "Epoch [412], val_loss: 0.7332, val_acc: 0.8016\n",
      "Epoch [413], val_loss: 0.7205, val_acc: 0.8062\n",
      "Epoch [414], val_loss: 0.7372, val_acc: 0.7958\n",
      "Epoch [415], val_loss: 0.7465, val_acc: 0.8016\n",
      "Epoch [416], val_loss: 0.7409, val_acc: 0.7969\n",
      "Epoch [417], val_loss: 0.7292, val_acc: 0.8012\n",
      "Epoch [418], val_loss: 0.7428, val_acc: 0.8047\n",
      "Epoch [419], val_loss: 0.7227, val_acc: 0.7963\n",
      "Epoch [420], val_loss: 0.7218, val_acc: 0.8140\n",
      "Epoch [421], val_loss: 0.7422, val_acc: 0.7933\n",
      "Epoch [422], val_loss: 0.7270, val_acc: 0.8071\n",
      "Epoch [423], val_loss: 0.7397, val_acc: 0.7929\n",
      "Epoch [424], val_loss: 0.7288, val_acc: 0.8007\n",
      "Epoch [425], val_loss: 0.7372, val_acc: 0.8021\n",
      "Epoch [426], val_loss: 0.7541, val_acc: 0.8047\n",
      "Epoch [427], val_loss: 0.7577, val_acc: 0.8125\n",
      "Epoch [428], val_loss: 0.7504, val_acc: 0.7993\n",
      "Epoch [429], val_loss: 0.7394, val_acc: 0.8007\n",
      "Epoch [430], val_loss: 0.7564, val_acc: 0.7979\n",
      "Epoch [431], val_loss: 0.7372, val_acc: 0.8081\n",
      "Epoch [432], val_loss: 0.7526, val_acc: 0.7974\n",
      "Epoch [433], val_loss: 0.7139, val_acc: 0.8179\n",
      "Epoch [434], val_loss: 0.7428, val_acc: 0.8081\n",
      "Epoch [435], val_loss: 0.7989, val_acc: 0.7943\n",
      "Epoch [436], val_loss: 0.7457, val_acc: 0.8002\n",
      "Epoch [437], val_loss: 0.7359, val_acc: 0.8135\n",
      "Epoch [438], val_loss: 0.7904, val_acc: 0.7982\n",
      "Epoch [439], val_loss: 0.8101, val_acc: 0.7676\n",
      "Epoch [440], val_loss: 0.7498, val_acc: 0.8012\n",
      "Epoch [441], val_loss: 0.7830, val_acc: 0.8091\n",
      "Epoch [442], val_loss: 0.8507, val_acc: 0.8008\n",
      "Epoch [443], val_loss: 0.7762, val_acc: 0.7815\n",
      "Epoch [444], val_loss: 0.7681, val_acc: 0.7953\n",
      "Epoch [445], val_loss: 0.7285, val_acc: 0.8071\n",
      "Epoch [446], val_loss: 0.7568, val_acc: 0.7948\n",
      "Epoch [447], val_loss: 0.7677, val_acc: 0.7865\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[477], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history1 \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[475], line 18\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, lr, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m## Validation phase\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mepoch_end(epoch, result)\n\u001b[1;32m     20\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[475], line 2\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, val_loader)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model, val_loader):\n\u001b[0;32m----> 2\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mvalidation_step(batch) \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_loader]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(model\u001b[38;5;241m.\u001b[39mvalidation_epoch_end(outputs))\n",
      "Cell \u001b[0;32mIn[475], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model, val_loader):\n\u001b[0;32m----> 2\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mvalidation_step(batch) \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_loader]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(model\u001b[38;5;241m.\u001b[39mvalidation_epoch_end(outputs))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[451], line 26\u001b[0m, in \u001b[0;36mCustomDatasetFromDepthImages.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     24\u001b[0m single_image_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_arr[index]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Open image\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m img_as_img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_image_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m img_as_img \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000.0\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Transform image to tensor\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/format.py:789\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 789\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;66;03m# Now read the actual data.\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history1 = fit(10000, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "0b42bfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.6952, val_acc: 0.7827\n",
      "Epoch [1], val_loss: 1.6815, val_acc: 0.7943\n",
      "Epoch [2], val_loss: 1.6923, val_acc: 0.7932\n",
      "Epoch [3], val_loss: 1.6699, val_acc: 0.7896\n",
      "Epoch [4], val_loss: 1.6808, val_acc: 0.7944\n",
      "Epoch [5], val_loss: 1.7298, val_acc: 0.7818\n",
      "Epoch [6], val_loss: 1.6911, val_acc: 0.7941\n",
      "Epoch [7], val_loss: 1.6855, val_acc: 0.7930\n",
      "Epoch [8], val_loss: 1.6767, val_acc: 0.7746\n",
      "Epoch [9], val_loss: 1.7163, val_acc: 0.7931\n",
      "Epoch [10], val_loss: 1.6980, val_acc: 0.7866\n",
      "Epoch [11], val_loss: 1.7058, val_acc: 0.7951\n",
      "Epoch [12], val_loss: 1.6654, val_acc: 0.7893\n",
      "Epoch [13], val_loss: 1.7004, val_acc: 0.7858\n",
      "Epoch [14], val_loss: 1.6968, val_acc: 0.7896\n",
      "Epoch [15], val_loss: 1.7330, val_acc: 0.7930\n",
      "Epoch [16], val_loss: 1.7708, val_acc: 0.7712\n",
      "Epoch [17], val_loss: 1.6767, val_acc: 0.7903\n",
      "Epoch [18], val_loss: 1.6999, val_acc: 0.7896\n",
      "Epoch [19], val_loss: 1.7153, val_acc: 0.7918\n",
      "Epoch [20], val_loss: 1.6677, val_acc: 0.7932\n",
      "Epoch [21], val_loss: 1.6996, val_acc: 0.7881\n",
      "Epoch [22], val_loss: 1.6954, val_acc: 0.7877\n",
      "Epoch [23], val_loss: 1.6945, val_acc: 0.7917\n",
      "Epoch [24], val_loss: 1.7505, val_acc: 0.7922\n",
      "Epoch [25], val_loss: 1.7315, val_acc: 0.7972\n",
      "Epoch [26], val_loss: 1.7320, val_acc: 0.7900\n",
      "Epoch [27], val_loss: 1.7424, val_acc: 0.7925\n",
      "Epoch [28], val_loss: 1.7385, val_acc: 0.7958\n",
      "Epoch [29], val_loss: 1.6997, val_acc: 0.7911\n",
      "Epoch [30], val_loss: 1.7362, val_acc: 0.7938\n",
      "Epoch [31], val_loss: 1.7440, val_acc: 0.7933\n",
      "Epoch [32], val_loss: 1.7452, val_acc: 0.7791\n",
      "Epoch [33], val_loss: 1.8379, val_acc: 0.7848\n",
      "Epoch [34], val_loss: 1.7394, val_acc: 0.7898\n",
      "Epoch [35], val_loss: 1.7176, val_acc: 0.7880\n",
      "Epoch [36], val_loss: 1.7350, val_acc: 0.7951\n",
      "Epoch [37], val_loss: 1.7569, val_acc: 0.7979\n",
      "Epoch [38], val_loss: 1.7277, val_acc: 0.7976\n",
      "Epoch [39], val_loss: 1.7592, val_acc: 0.7891\n",
      "Epoch [40], val_loss: 1.7235, val_acc: 0.7932\n",
      "Epoch [41], val_loss: 1.7585, val_acc: 0.7850\n",
      "Epoch [42], val_loss: 1.7384, val_acc: 0.7896\n",
      "Epoch [43], val_loss: 1.7607, val_acc: 0.7795\n",
      "Epoch [44], val_loss: 1.7443, val_acc: 0.7864\n",
      "Epoch [45], val_loss: 1.7647, val_acc: 0.7919\n",
      "Epoch [46], val_loss: 1.7962, val_acc: 0.7889\n",
      "Epoch [47], val_loss: 1.7317, val_acc: 0.7912\n",
      "Epoch [48], val_loss: 1.7755, val_acc: 0.7902\n",
      "Epoch [49], val_loss: 1.8175, val_acc: 0.7930\n",
      "Epoch [50], val_loss: 1.7537, val_acc: 0.7898\n",
      "Epoch [51], val_loss: 1.7893, val_acc: 0.7873\n",
      "Epoch [52], val_loss: 1.7847, val_acc: 0.7856\n",
      "Epoch [53], val_loss: 1.7705, val_acc: 0.7853\n",
      "Epoch [54], val_loss: 1.7551, val_acc: 0.7925\n",
      "Epoch [55], val_loss: 1.7982, val_acc: 0.7838\n",
      "Epoch [56], val_loss: 1.7454, val_acc: 0.7976\n",
      "Epoch [57], val_loss: 1.7707, val_acc: 0.7932\n",
      "Epoch [58], val_loss: 1.7679, val_acc: 0.7883\n",
      "Epoch [59], val_loss: 1.7819, val_acc: 0.7878\n",
      "Epoch [60], val_loss: 1.7599, val_acc: 0.7900\n",
      "Epoch [61], val_loss: 1.7750, val_acc: 0.7906\n",
      "Epoch [62], val_loss: 1.7901, val_acc: 0.7874\n",
      "Epoch [63], val_loss: 1.7998, val_acc: 0.7987\n",
      "Epoch [64], val_loss: 1.7723, val_acc: 0.7872\n",
      "Epoch [65], val_loss: 1.7816, val_acc: 0.7942\n",
      "Epoch [66], val_loss: 1.8053, val_acc: 0.7976\n",
      "Epoch [67], val_loss: 1.7548, val_acc: 0.7923\n",
      "Epoch [68], val_loss: 1.7967, val_acc: 0.7883\n",
      "Epoch [69], val_loss: 1.8090, val_acc: 0.7882\n",
      "Epoch [70], val_loss: 1.7927, val_acc: 0.7906\n",
      "Epoch [71], val_loss: 1.7464, val_acc: 0.7969\n",
      "Epoch [72], val_loss: 1.8349, val_acc: 0.7906\n",
      "Epoch [73], val_loss: 1.7565, val_acc: 0.7932\n",
      "Epoch [74], val_loss: 1.7795, val_acc: 0.7913\n",
      "Epoch [75], val_loss: 1.8008, val_acc: 0.7897\n",
      "Epoch [76], val_loss: 1.7744, val_acc: 0.7969\n",
      "Epoch [77], val_loss: 1.8163, val_acc: 0.7792\n",
      "Epoch [78], val_loss: 1.7803, val_acc: 0.7947\n",
      "Epoch [79], val_loss: 1.8464, val_acc: 0.7896\n",
      "Epoch [80], val_loss: 1.7720, val_acc: 0.7904\n",
      "Epoch [81], val_loss: 1.7884, val_acc: 0.7934\n",
      "Epoch [82], val_loss: 1.7803, val_acc: 0.7876\n",
      "Epoch [83], val_loss: 1.8121, val_acc: 0.7965\n",
      "Epoch [84], val_loss: 1.8182, val_acc: 0.7898\n",
      "Epoch [85], val_loss: 1.8333, val_acc: 0.7815\n",
      "Epoch [86], val_loss: 1.8915, val_acc: 0.7824\n",
      "Epoch [87], val_loss: 1.8929, val_acc: 0.7872\n",
      "Epoch [88], val_loss: 1.8280, val_acc: 0.7927\n",
      "Epoch [89], val_loss: 1.7774, val_acc: 0.7954\n",
      "Epoch [90], val_loss: 1.8312, val_acc: 0.7772\n",
      "Epoch [91], val_loss: 1.8358, val_acc: 0.7933\n",
      "Epoch [92], val_loss: 1.8015, val_acc: 0.7891\n",
      "Epoch [93], val_loss: 1.8170, val_acc: 0.7935\n",
      "Epoch [94], val_loss: 1.8584, val_acc: 0.7913\n",
      "Epoch [95], val_loss: 1.8209, val_acc: 0.7900\n",
      "Epoch [96], val_loss: 1.8403, val_acc: 0.7921\n",
      "Epoch [97], val_loss: 1.8987, val_acc: 0.7721\n",
      "Epoch [98], val_loss: 1.8081, val_acc: 0.7875\n",
      "Epoch [99], val_loss: 1.8780, val_acc: 0.7869\n",
      "Epoch [100], val_loss: 1.8057, val_acc: 0.7939\n",
      "Epoch [101], val_loss: 1.8494, val_acc: 0.7822\n",
      "Epoch [102], val_loss: 1.8009, val_acc: 0.7965\n",
      "Epoch [103], val_loss: 1.8098, val_acc: 0.7918\n",
      "Epoch [104], val_loss: 1.8474, val_acc: 0.7908\n",
      "Epoch [105], val_loss: 1.8260, val_acc: 0.7947\n",
      "Epoch [106], val_loss: 1.8092, val_acc: 0.7944\n",
      "Epoch [107], val_loss: 1.8440, val_acc: 0.7888\n",
      "Epoch [108], val_loss: 1.8354, val_acc: 0.7900\n",
      "Epoch [109], val_loss: 1.8358, val_acc: 0.7960\n",
      "Epoch [110], val_loss: 1.8555, val_acc: 0.7947\n",
      "Epoch [111], val_loss: 1.8547, val_acc: 0.7842\n",
      "Epoch [112], val_loss: 1.8458, val_acc: 0.7998\n",
      "Epoch [113], val_loss: 1.8828, val_acc: 0.7846\n",
      "Epoch [114], val_loss: 1.8657, val_acc: 0.7900\n",
      "Epoch [115], val_loss: 1.8879, val_acc: 0.7872\n",
      "Epoch [116], val_loss: 1.8144, val_acc: 0.7957\n",
      "Epoch [117], val_loss: 1.8086, val_acc: 0.7965\n",
      "Epoch [118], val_loss: 1.8305, val_acc: 0.7898\n",
      "Epoch [119], val_loss: 1.8491, val_acc: 0.7915\n",
      "Epoch [120], val_loss: 1.8270, val_acc: 0.7965\n",
      "Epoch [121], val_loss: 1.8633, val_acc: 0.7912\n",
      "Epoch [122], val_loss: 1.8435, val_acc: 0.7954\n",
      "Epoch [123], val_loss: 1.8711, val_acc: 0.7903\n",
      "Epoch [124], val_loss: 1.8567, val_acc: 0.7915\n",
      "Epoch [125], val_loss: 1.8862, val_acc: 0.7871\n",
      "Epoch [126], val_loss: 1.8969, val_acc: 0.7933\n",
      "Epoch [127], val_loss: 1.8856, val_acc: 0.7891\n",
      "Epoch [128], val_loss: 1.8870, val_acc: 0.7888\n",
      "Epoch [129], val_loss: 1.8790, val_acc: 0.7909\n",
      "Epoch [130], val_loss: 1.8574, val_acc: 0.7913\n",
      "Epoch [131], val_loss: 1.8996, val_acc: 0.7863\n",
      "Epoch [132], val_loss: 1.8610, val_acc: 0.7907\n",
      "Epoch [133], val_loss: 1.8968, val_acc: 0.7910\n",
      "Epoch [134], val_loss: 1.9009, val_acc: 0.7851\n",
      "Epoch [135], val_loss: 1.9228, val_acc: 0.7858\n",
      "Epoch [136], val_loss: 1.8780, val_acc: 0.7919\n",
      "Epoch [137], val_loss: 1.8662, val_acc: 0.7912\n",
      "Epoch [138], val_loss: 1.8730, val_acc: 0.7853\n",
      "Epoch [139], val_loss: 1.8503, val_acc: 0.7951\n",
      "Epoch [140], val_loss: 1.8388, val_acc: 0.7931\n",
      "Epoch [141], val_loss: 1.8567, val_acc: 0.7983\n",
      "Epoch [142], val_loss: 1.8580, val_acc: 0.7924\n",
      "Epoch [143], val_loss: 1.8852, val_acc: 0.7988\n",
      "Epoch [144], val_loss: 1.8826, val_acc: 0.7954\n",
      "Epoch [145], val_loss: 1.8909, val_acc: 0.7846\n",
      "Epoch [146], val_loss: 1.8896, val_acc: 0.7951\n",
      "Epoch [147], val_loss: 1.8966, val_acc: 0.7992\n",
      "Epoch [148], val_loss: 1.8901, val_acc: 0.7858\n",
      "Epoch [149], val_loss: 1.8947, val_acc: 0.7911\n",
      "Epoch [150], val_loss: 1.8831, val_acc: 0.7951\n",
      "Epoch [151], val_loss: 1.9445, val_acc: 0.7833\n",
      "Epoch [152], val_loss: 1.9261, val_acc: 0.7884\n",
      "Epoch [153], val_loss: 1.9840, val_acc: 0.7750\n",
      "Epoch [154], val_loss: 1.9537, val_acc: 0.7790\n",
      "Epoch [155], val_loss: 1.9009, val_acc: 0.7941\n",
      "Epoch [156], val_loss: 1.9041, val_acc: 0.7880\n",
      "Epoch [157], val_loss: 1.9120, val_acc: 0.7897\n",
      "Epoch [158], val_loss: 1.8983, val_acc: 0.7860\n",
      "Epoch [159], val_loss: 1.8906, val_acc: 0.7915\n",
      "Epoch [160], val_loss: 1.8683, val_acc: 0.7923\n",
      "Epoch [161], val_loss: 1.9066, val_acc: 0.7871\n",
      "Epoch [162], val_loss: 1.8855, val_acc: 0.7956\n",
      "Epoch [163], val_loss: 1.9648, val_acc: 0.7827\n",
      "Epoch [164], val_loss: 1.9213, val_acc: 0.7881\n",
      "Epoch [165], val_loss: 1.9167, val_acc: 0.7910\n",
      "Epoch [166], val_loss: 1.8979, val_acc: 0.7882\n",
      "Epoch [167], val_loss: 1.8828, val_acc: 0.7894\n",
      "Epoch [168], val_loss: 1.9181, val_acc: 0.7918\n",
      "Epoch [169], val_loss: 1.8897, val_acc: 0.7918\n",
      "Epoch [170], val_loss: 1.9330, val_acc: 0.7891\n",
      "Epoch [171], val_loss: 1.9684, val_acc: 0.7814\n",
      "Epoch [172], val_loss: 1.9266, val_acc: 0.7938\n",
      "Epoch [173], val_loss: 1.9804, val_acc: 0.7932\n",
      "Epoch [174], val_loss: 1.9570, val_acc: 0.7988\n",
      "Epoch [175], val_loss: 1.9729, val_acc: 0.7979\n",
      "Epoch [176], val_loss: 2.0009, val_acc: 0.7828\n",
      "Epoch [177], val_loss: 1.9963, val_acc: 0.7902\n",
      "Epoch [178], val_loss: 1.9552, val_acc: 0.7891\n",
      "Epoch [179], val_loss: 1.9411, val_acc: 0.7898\n",
      "Epoch [180], val_loss: 1.9390, val_acc: 0.7944\n",
      "Epoch [181], val_loss: 1.9657, val_acc: 0.7843\n",
      "Epoch [182], val_loss: 1.9268, val_acc: 0.7913\n",
      "Epoch [183], val_loss: 1.9069, val_acc: 0.7923\n",
      "Epoch [184], val_loss: 1.9109, val_acc: 0.7966\n",
      "Epoch [185], val_loss: 1.9324, val_acc: 0.7846\n",
      "Epoch [186], val_loss: 1.9613, val_acc: 0.7955\n",
      "Epoch [187], val_loss: 1.9313, val_acc: 0.7933\n",
      "Epoch [188], val_loss: 1.9299, val_acc: 0.7882\n",
      "Epoch [189], val_loss: 1.9249, val_acc: 0.7942\n",
      "Epoch [190], val_loss: 1.9453, val_acc: 0.7941\n",
      "Epoch [191], val_loss: 1.9701, val_acc: 0.7897\n",
      "Epoch [192], val_loss: 1.9616, val_acc: 0.7932\n",
      "Epoch [193], val_loss: 1.9801, val_acc: 0.7789\n",
      "Epoch [194], val_loss: 1.9658, val_acc: 0.7952\n",
      "Epoch [195], val_loss: 1.9818, val_acc: 0.7890\n",
      "Epoch [196], val_loss: 1.9707, val_acc: 0.7950\n",
      "Epoch [197], val_loss: 1.9703, val_acc: 0.7909\n",
      "Epoch [198], val_loss: 2.0187, val_acc: 0.7874\n",
      "Epoch [199], val_loss: 1.9564, val_acc: 0.7981\n",
      "Epoch [200], val_loss: 1.9623, val_acc: 0.7903\n",
      "Epoch [201], val_loss: 1.9552, val_acc: 0.7934\n",
      "Epoch [202], val_loss: 1.9933, val_acc: 0.7903\n",
      "Epoch [203], val_loss: 1.9730, val_acc: 0.7896\n",
      "Epoch [204], val_loss: 1.9820, val_acc: 0.7886\n",
      "Epoch [205], val_loss: 1.9873, val_acc: 0.7849\n",
      "Epoch [206], val_loss: 1.9870, val_acc: 0.7909\n",
      "Epoch [207], val_loss: 1.9602, val_acc: 0.7925\n",
      "Epoch [208], val_loss: 1.9854, val_acc: 0.7917\n",
      "Epoch [209], val_loss: 1.9896, val_acc: 0.7882\n",
      "Epoch [210], val_loss: 2.0040, val_acc: 0.7837\n",
      "Epoch [211], val_loss: 2.0541, val_acc: 0.7884\n",
      "Epoch [212], val_loss: 1.9781, val_acc: 0.7887\n",
      "Epoch [213], val_loss: 1.9839, val_acc: 0.7935\n",
      "Epoch [214], val_loss: 2.0379, val_acc: 0.7923\n",
      "Epoch [215], val_loss: 2.0456, val_acc: 0.7847\n",
      "Epoch [216], val_loss: 2.0778, val_acc: 0.7829\n",
      "Epoch [217], val_loss: 1.9688, val_acc: 0.7911\n",
      "Epoch [218], val_loss: 1.9805, val_acc: 0.7939\n",
      "Epoch [219], val_loss: 2.0023, val_acc: 0.7979\n",
      "Epoch [220], val_loss: 2.0029, val_acc: 0.7946\n",
      "Epoch [221], val_loss: 1.9933, val_acc: 0.7945\n",
      "Epoch [222], val_loss: 1.9722, val_acc: 0.7862\n",
      "Epoch [223], val_loss: 2.0366, val_acc: 0.7804\n",
      "Epoch [224], val_loss: 2.0226, val_acc: 0.7954\n",
      "Epoch [225], val_loss: 2.0277, val_acc: 0.7951\n",
      "Epoch [226], val_loss: 2.0333, val_acc: 0.7878\n",
      "Epoch [227], val_loss: 2.0444, val_acc: 0.7804\n",
      "Epoch [228], val_loss: 1.9694, val_acc: 0.7982\n",
      "Epoch [229], val_loss: 2.0065, val_acc: 0.7938\n",
      "Epoch [230], val_loss: 1.9876, val_acc: 0.7918\n",
      "Epoch [231], val_loss: 2.0446, val_acc: 0.7880\n",
      "Epoch [232], val_loss: 2.0123, val_acc: 0.7944\n",
      "Epoch [233], val_loss: 2.0234, val_acc: 0.7964\n",
      "Epoch [234], val_loss: 2.0054, val_acc: 0.7870\n",
      "Epoch [235], val_loss: 2.0072, val_acc: 0.7857\n",
      "Epoch [236], val_loss: 2.0397, val_acc: 0.7864\n",
      "Epoch [237], val_loss: 2.1013, val_acc: 0.7863\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[407], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history2 \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[404], line 12\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, lr, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m## Training Phas\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 12\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[402], line 27\u001b[0m, in \u001b[0;36mMnistModel.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m     26\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 27\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m## Generate predictions\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out, labels) \u001b[38;5;66;03m## Calculate the loss\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[402], line 17\u001b[0m, in \u001b[0;36mMnistModel.forward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xb):\n\u001b[1;32m     16\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(xb)\n\u001b[0;32m---> 17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m(out)\n\u001b[1;32m     18\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(out)\n\u001b[1;32m     19\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1696\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1698\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history2 = fit(1000, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "b8edf808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.4581, val_acc: 0.4634\n",
      "Epoch [1], val_loss: 1.4580, val_acc: 0.4639\n",
      "Epoch [2], val_loss: 1.4579, val_acc: 0.4639\n",
      "Epoch [3], val_loss: 1.4578, val_acc: 0.4639\n",
      "Epoch [4], val_loss: 1.4577, val_acc: 0.4639\n",
      "Epoch [5], val_loss: 1.4576, val_acc: 0.4639\n",
      "Epoch [6], val_loss: 1.4575, val_acc: 0.4639\n",
      "Epoch [7], val_loss: 1.4574, val_acc: 0.4639\n",
      "Epoch [8], val_loss: 1.4573, val_acc: 0.4639\n",
      "Epoch [9], val_loss: 1.4572, val_acc: 0.4639\n",
      "Epoch [10], val_loss: 1.4571, val_acc: 0.4639\n",
      "Epoch [11], val_loss: 1.4570, val_acc: 0.4639\n",
      "Epoch [12], val_loss: 1.4569, val_acc: 0.4639\n",
      "Epoch [13], val_loss: 1.4568, val_acc: 0.4639\n",
      "Epoch [14], val_loss: 1.4567, val_acc: 0.4639\n",
      "Epoch [15], val_loss: 1.4566, val_acc: 0.4639\n",
      "Epoch [16], val_loss: 1.4565, val_acc: 0.4639\n",
      "Epoch [17], val_loss: 1.4563, val_acc: 0.4639\n",
      "Epoch [18], val_loss: 1.4562, val_acc: 0.4639\n",
      "Epoch [19], val_loss: 1.4561, val_acc: 0.4639\n",
      "Epoch [20], val_loss: 1.4560, val_acc: 0.4639\n",
      "Epoch [21], val_loss: 1.4559, val_acc: 0.4639\n",
      "Epoch [22], val_loss: 1.4558, val_acc: 0.4639\n",
      "Epoch [23], val_loss: 1.4557, val_acc: 0.4634\n",
      "Epoch [24], val_loss: 1.4556, val_acc: 0.4634\n",
      "Epoch [25], val_loss: 1.4555, val_acc: 0.4634\n",
      "Epoch [26], val_loss: 1.4554, val_acc: 0.4634\n",
      "Epoch [27], val_loss: 1.4553, val_acc: 0.4634\n",
      "Epoch [28], val_loss: 1.4552, val_acc: 0.4634\n",
      "Epoch [29], val_loss: 1.4551, val_acc: 0.4634\n",
      "Epoch [30], val_loss: 1.4550, val_acc: 0.4634\n",
      "Epoch [31], val_loss: 1.4549, val_acc: 0.4639\n",
      "Epoch [32], val_loss: 1.4548, val_acc: 0.4639\n",
      "Epoch [33], val_loss: 1.4547, val_acc: 0.4639\n",
      "Epoch [34], val_loss: 1.4546, val_acc: 0.4639\n",
      "Epoch [35], val_loss: 1.4545, val_acc: 0.4639\n",
      "Epoch [36], val_loss: 1.4544, val_acc: 0.4639\n",
      "Epoch [37], val_loss: 1.4543, val_acc: 0.4639\n",
      "Epoch [38], val_loss: 1.4542, val_acc: 0.4639\n",
      "Epoch [39], val_loss: 1.4541, val_acc: 0.4639\n",
      "Epoch [40], val_loss: 1.4540, val_acc: 0.4639\n",
      "Epoch [41], val_loss: 1.4539, val_acc: 0.4639\n",
      "Epoch [42], val_loss: 1.4538, val_acc: 0.4639\n",
      "Epoch [43], val_loss: 1.4538, val_acc: 0.4639\n",
      "Epoch [44], val_loss: 1.4537, val_acc: 0.4639\n",
      "Epoch [45], val_loss: 1.4536, val_acc: 0.4639\n",
      "Epoch [46], val_loss: 1.4535, val_acc: 0.4639\n",
      "Epoch [47], val_loss: 1.4534, val_acc: 0.4639\n",
      "Epoch [48], val_loss: 1.4533, val_acc: 0.4639\n",
      "Epoch [49], val_loss: 1.4532, val_acc: 0.4639\n",
      "Epoch [50], val_loss: 1.4531, val_acc: 0.4639\n",
      "Epoch [51], val_loss: 1.4530, val_acc: 0.4639\n",
      "Epoch [52], val_loss: 1.4529, val_acc: 0.4639\n",
      "Epoch [53], val_loss: 1.4528, val_acc: 0.4639\n",
      "Epoch [54], val_loss: 1.4527, val_acc: 0.4639\n",
      "Epoch [55], val_loss: 1.4526, val_acc: 0.4639\n",
      "Epoch [56], val_loss: 1.4525, val_acc: 0.4639\n",
      "Epoch [57], val_loss: 1.4524, val_acc: 0.4639\n",
      "Epoch [58], val_loss: 1.4523, val_acc: 0.4639\n",
      "Epoch [59], val_loss: 1.4523, val_acc: 0.4644\n",
      "Epoch [60], val_loss: 1.4522, val_acc: 0.4644\n",
      "Epoch [61], val_loss: 1.4521, val_acc: 0.4644\n",
      "Epoch [62], val_loss: 1.4520, val_acc: 0.4644\n",
      "Epoch [63], val_loss: 1.4519, val_acc: 0.4644\n",
      "Epoch [64], val_loss: 1.4518, val_acc: 0.4644\n",
      "Epoch [65], val_loss: 1.4517, val_acc: 0.4644\n",
      "Epoch [66], val_loss: 1.4516, val_acc: 0.4644\n",
      "Epoch [67], val_loss: 1.4515, val_acc: 0.4644\n",
      "Epoch [68], val_loss: 1.4514, val_acc: 0.4639\n",
      "Epoch [69], val_loss: 1.4514, val_acc: 0.4639\n",
      "Epoch [70], val_loss: 1.4513, val_acc: 0.4639\n",
      "Epoch [71], val_loss: 1.4512, val_acc: 0.4639\n",
      "Epoch [72], val_loss: 1.4511, val_acc: 0.4644\n",
      "Epoch [73], val_loss: 1.4510, val_acc: 0.4644\n",
      "Epoch [74], val_loss: 1.4509, val_acc: 0.4644\n",
      "Epoch [75], val_loss: 1.4508, val_acc: 0.4644\n",
      "Epoch [76], val_loss: 1.4507, val_acc: 0.4644\n",
      "Epoch [77], val_loss: 1.4507, val_acc: 0.4644\n",
      "Epoch [78], val_loss: 1.4506, val_acc: 0.4644\n",
      "Epoch [79], val_loss: 1.4505, val_acc: 0.4639\n",
      "Epoch [80], val_loss: 1.4504, val_acc: 0.4639\n",
      "Epoch [81], val_loss: 1.4503, val_acc: 0.4639\n",
      "Epoch [82], val_loss: 1.4502, val_acc: 0.4634\n",
      "Epoch [83], val_loss: 1.4501, val_acc: 0.4629\n",
      "Epoch [84], val_loss: 1.4501, val_acc: 0.4629\n",
      "Epoch [85], val_loss: 1.4500, val_acc: 0.4629\n",
      "Epoch [86], val_loss: 1.4499, val_acc: 0.4629\n",
      "Epoch [87], val_loss: 1.4498, val_acc: 0.4629\n",
      "Epoch [88], val_loss: 1.4497, val_acc: 0.4629\n",
      "Epoch [89], val_loss: 1.4496, val_acc: 0.4629\n",
      "Epoch [90], val_loss: 1.4496, val_acc: 0.4629\n",
      "Epoch [91], val_loss: 1.4495, val_acc: 0.4629\n",
      "Epoch [92], val_loss: 1.4494, val_acc: 0.4629\n",
      "Epoch [93], val_loss: 1.4493, val_acc: 0.4629\n",
      "Epoch [94], val_loss: 1.4492, val_acc: 0.4629\n",
      "Epoch [95], val_loss: 1.4491, val_acc: 0.4629\n",
      "Epoch [96], val_loss: 1.4491, val_acc: 0.4629\n",
      "Epoch [97], val_loss: 1.4490, val_acc: 0.4629\n",
      "Epoch [98], val_loss: 1.4489, val_acc: 0.4629\n",
      "Epoch [99], val_loss: 1.4488, val_acc: 0.4629\n",
      "Epoch [100], val_loss: 1.4487, val_acc: 0.4629\n",
      "Epoch [101], val_loss: 1.4487, val_acc: 0.4629\n",
      "Epoch [102], val_loss: 1.4486, val_acc: 0.4624\n",
      "Epoch [103], val_loss: 1.4485, val_acc: 0.4619\n",
      "Epoch [104], val_loss: 1.4484, val_acc: 0.4624\n",
      "Epoch [105], val_loss: 1.4483, val_acc: 0.4629\n",
      "Epoch [106], val_loss: 1.4483, val_acc: 0.4624\n",
      "Epoch [107], val_loss: 1.4482, val_acc: 0.4624\n",
      "Epoch [108], val_loss: 1.4481, val_acc: 0.4624\n",
      "Epoch [109], val_loss: 1.4480, val_acc: 0.4624\n",
      "Epoch [110], val_loss: 1.4479, val_acc: 0.4624\n",
      "Epoch [111], val_loss: 1.4479, val_acc: 0.4624\n",
      "Epoch [112], val_loss: 1.4478, val_acc: 0.4624\n",
      "Epoch [113], val_loss: 1.4477, val_acc: 0.4624\n",
      "Epoch [114], val_loss: 1.4476, val_acc: 0.4619\n",
      "Epoch [115], val_loss: 1.4476, val_acc: 0.4619\n",
      "Epoch [116], val_loss: 1.4475, val_acc: 0.4619\n",
      "Epoch [117], val_loss: 1.4474, val_acc: 0.4619\n",
      "Epoch [118], val_loss: 1.4473, val_acc: 0.4619\n",
      "Epoch [119], val_loss: 1.4473, val_acc: 0.4619\n",
      "Epoch [120], val_loss: 1.4472, val_acc: 0.4619\n",
      "Epoch [121], val_loss: 1.4471, val_acc: 0.4619\n",
      "Epoch [122], val_loss: 1.4470, val_acc: 0.4619\n",
      "Epoch [123], val_loss: 1.4470, val_acc: 0.4619\n",
      "Epoch [124], val_loss: 1.4469, val_acc: 0.4619\n",
      "Epoch [125], val_loss: 1.4468, val_acc: 0.4619\n",
      "Epoch [126], val_loss: 1.4467, val_acc: 0.4619\n",
      "Epoch [127], val_loss: 1.4467, val_acc: 0.4619\n",
      "Epoch [128], val_loss: 1.4466, val_acc: 0.4619\n",
      "Epoch [129], val_loss: 1.4465, val_acc: 0.4619\n",
      "Epoch [130], val_loss: 1.4464, val_acc: 0.4619\n",
      "Epoch [131], val_loss: 1.4464, val_acc: 0.4619\n",
      "Epoch [132], val_loss: 1.4463, val_acc: 0.4619\n",
      "Epoch [133], val_loss: 1.4462, val_acc: 0.4619\n",
      "Epoch [134], val_loss: 1.4462, val_acc: 0.4624\n",
      "Epoch [135], val_loss: 1.4461, val_acc: 0.4624\n",
      "Epoch [136], val_loss: 1.4460, val_acc: 0.4629\n",
      "Epoch [137], val_loss: 1.4459, val_acc: 0.4624\n",
      "Epoch [138], val_loss: 1.4459, val_acc: 0.4629\n",
      "Epoch [139], val_loss: 1.4458, val_acc: 0.4629\n",
      "Epoch [140], val_loss: 1.4457, val_acc: 0.4629\n",
      "Epoch [141], val_loss: 1.4457, val_acc: 0.4629\n",
      "Epoch [142], val_loss: 1.4456, val_acc: 0.4629\n",
      "Epoch [143], val_loss: 1.4455, val_acc: 0.4629\n",
      "Epoch [144], val_loss: 1.4454, val_acc: 0.4629\n",
      "Epoch [145], val_loss: 1.4454, val_acc: 0.4629\n",
      "Epoch [146], val_loss: 1.4453, val_acc: 0.4629\n",
      "Epoch [147], val_loss: 1.4452, val_acc: 0.4629\n",
      "Epoch [148], val_loss: 1.4452, val_acc: 0.4629\n",
      "Epoch [149], val_loss: 1.4451, val_acc: 0.4629\n",
      "Epoch [150], val_loss: 1.4450, val_acc: 0.4629\n",
      "Epoch [151], val_loss: 1.4450, val_acc: 0.4634\n",
      "Epoch [152], val_loss: 1.4449, val_acc: 0.4634\n",
      "Epoch [153], val_loss: 1.4448, val_acc: 0.4634\n",
      "Epoch [154], val_loss: 1.4448, val_acc: 0.4634\n",
      "Epoch [155], val_loss: 1.4447, val_acc: 0.4634\n",
      "Epoch [156], val_loss: 1.4446, val_acc: 0.4634\n",
      "Epoch [157], val_loss: 1.4446, val_acc: 0.4634\n",
      "Epoch [158], val_loss: 1.4445, val_acc: 0.4634\n",
      "Epoch [159], val_loss: 1.4444, val_acc: 0.4634\n",
      "Epoch [160], val_loss: 1.4444, val_acc: 0.4634\n",
      "Epoch [161], val_loss: 1.4443, val_acc: 0.4634\n",
      "Epoch [162], val_loss: 1.4442, val_acc: 0.4634\n",
      "Epoch [163], val_loss: 1.4442, val_acc: 0.4627\n",
      "Epoch [164], val_loss: 1.4441, val_acc: 0.4627\n",
      "Epoch [165], val_loss: 1.4440, val_acc: 0.4627\n",
      "Epoch [166], val_loss: 1.4440, val_acc: 0.4627\n",
      "Epoch [167], val_loss: 1.4439, val_acc: 0.4627\n",
      "Epoch [168], val_loss: 1.4438, val_acc: 0.4627\n",
      "Epoch [169], val_loss: 1.4438, val_acc: 0.4627\n",
      "Epoch [170], val_loss: 1.4437, val_acc: 0.4627\n",
      "Epoch [171], val_loss: 1.4436, val_acc: 0.4627\n",
      "Epoch [172], val_loss: 1.4436, val_acc: 0.4627\n",
      "Epoch [173], val_loss: 1.4435, val_acc: 0.4627\n",
      "Epoch [174], val_loss: 1.4435, val_acc: 0.4627\n",
      "Epoch [175], val_loss: 1.4434, val_acc: 0.4627\n",
      "Epoch [176], val_loss: 1.4433, val_acc: 0.4627\n",
      "Epoch [177], val_loss: 1.4433, val_acc: 0.4632\n",
      "Epoch [178], val_loss: 1.4432, val_acc: 0.4627\n",
      "Epoch [179], val_loss: 1.4431, val_acc: 0.4632\n",
      "Epoch [180], val_loss: 1.4431, val_acc: 0.4627\n",
      "Epoch [181], val_loss: 1.4430, val_acc: 0.4627\n",
      "Epoch [182], val_loss: 1.4430, val_acc: 0.4627\n",
      "Epoch [183], val_loss: 1.4429, val_acc: 0.4627\n",
      "Epoch [184], val_loss: 1.4428, val_acc: 0.4627\n",
      "Epoch [185], val_loss: 1.4428, val_acc: 0.4627\n",
      "Epoch [186], val_loss: 1.4427, val_acc: 0.4627\n",
      "Epoch [187], val_loss: 1.4427, val_acc: 0.4627\n",
      "Epoch [188], val_loss: 1.4426, val_acc: 0.4627\n",
      "Epoch [189], val_loss: 1.4425, val_acc: 0.4627\n",
      "Epoch [190], val_loss: 1.4425, val_acc: 0.4627\n",
      "Epoch [191], val_loss: 1.4424, val_acc: 0.4627\n",
      "Epoch [192], val_loss: 1.4424, val_acc: 0.4627\n",
      "Epoch [193], val_loss: 1.4423, val_acc: 0.4627\n",
      "Epoch [194], val_loss: 1.4422, val_acc: 0.4627\n",
      "Epoch [195], val_loss: 1.4422, val_acc: 0.4627\n",
      "Epoch [196], val_loss: 1.4421, val_acc: 0.4627\n",
      "Epoch [197], val_loss: 1.4421, val_acc: 0.4627\n",
      "Epoch [198], val_loss: 1.4420, val_acc: 0.4627\n",
      "Epoch [199], val_loss: 1.4419, val_acc: 0.4627\n",
      "Epoch [200], val_loss: 1.4419, val_acc: 0.4627\n",
      "Epoch [201], val_loss: 1.4418, val_acc: 0.4627\n",
      "Epoch [202], val_loss: 1.4418, val_acc: 0.4627\n",
      "Epoch [203], val_loss: 1.4417, val_acc: 0.4627\n",
      "Epoch [204], val_loss: 1.4417, val_acc: 0.4627\n",
      "Epoch [205], val_loss: 1.4416, val_acc: 0.4627\n",
      "Epoch [206], val_loss: 1.4415, val_acc: 0.4627\n",
      "Epoch [207], val_loss: 1.4415, val_acc: 0.4627\n",
      "Epoch [208], val_loss: 1.4414, val_acc: 0.4627\n",
      "Epoch [209], val_loss: 1.4414, val_acc: 0.4627\n",
      "Epoch [210], val_loss: 1.4413, val_acc: 0.4627\n",
      "Epoch [211], val_loss: 1.4413, val_acc: 0.4627\n",
      "Epoch [212], val_loss: 1.4412, val_acc: 0.4627\n",
      "Epoch [213], val_loss: 1.4411, val_acc: 0.4627\n",
      "Epoch [214], val_loss: 1.4411, val_acc: 0.4627\n",
      "Epoch [215], val_loss: 1.4410, val_acc: 0.4627\n",
      "Epoch [216], val_loss: 1.4410, val_acc: 0.4627\n",
      "Epoch [217], val_loss: 1.4409, val_acc: 0.4627\n",
      "Epoch [218], val_loss: 1.4409, val_acc: 0.4627\n",
      "Epoch [219], val_loss: 1.4408, val_acc: 0.4627\n",
      "Epoch [220], val_loss: 1.4408, val_acc: 0.4632\n",
      "Epoch [221], val_loss: 1.4407, val_acc: 0.4632\n",
      "Epoch [222], val_loss: 1.4406, val_acc: 0.4632\n",
      "Epoch [223], val_loss: 1.4406, val_acc: 0.4632\n",
      "Epoch [224], val_loss: 1.4405, val_acc: 0.4632\n",
      "Epoch [225], val_loss: 1.4405, val_acc: 0.4627\n",
      "Epoch [226], val_loss: 1.4404, val_acc: 0.4627\n",
      "Epoch [227], val_loss: 1.4404, val_acc: 0.4627\n",
      "Epoch [228], val_loss: 1.4403, val_acc: 0.4627\n",
      "Epoch [229], val_loss: 1.4403, val_acc: 0.4627\n",
      "Epoch [230], val_loss: 1.4402, val_acc: 0.4627\n",
      "Epoch [231], val_loss: 1.4402, val_acc: 0.4627\n",
      "Epoch [232], val_loss: 1.4401, val_acc: 0.4627\n",
      "Epoch [233], val_loss: 1.4401, val_acc: 0.4627\n",
      "Epoch [234], val_loss: 1.4400, val_acc: 0.4627\n",
      "Epoch [235], val_loss: 1.4400, val_acc: 0.4627\n",
      "Epoch [236], val_loss: 1.4399, val_acc: 0.4627\n",
      "Epoch [237], val_loss: 1.4399, val_acc: 0.4627\n",
      "Epoch [238], val_loss: 1.4398, val_acc: 0.4627\n",
      "Epoch [239], val_loss: 1.4398, val_acc: 0.4627\n",
      "Epoch [240], val_loss: 1.4397, val_acc: 0.4627\n",
      "Epoch [241], val_loss: 1.4397, val_acc: 0.4627\n",
      "Epoch [242], val_loss: 1.4396, val_acc: 0.4627\n",
      "Epoch [243], val_loss: 1.4395, val_acc: 0.4627\n",
      "Epoch [244], val_loss: 1.4395, val_acc: 0.4627\n",
      "Epoch [245], val_loss: 1.4394, val_acc: 0.4627\n",
      "Epoch [246], val_loss: 1.4394, val_acc: 0.4627\n",
      "Epoch [247], val_loss: 1.4393, val_acc: 0.4627\n",
      "Epoch [248], val_loss: 1.4393, val_acc: 0.4627\n",
      "Epoch [249], val_loss: 1.4392, val_acc: 0.4627\n",
      "Epoch [250], val_loss: 1.4392, val_acc: 0.4627\n",
      "Epoch [251], val_loss: 1.4391, val_acc: 0.4627\n",
      "Epoch [252], val_loss: 1.4391, val_acc: 0.4627\n",
      "Epoch [253], val_loss: 1.4390, val_acc: 0.4627\n",
      "Epoch [254], val_loss: 1.4390, val_acc: 0.4627\n",
      "Epoch [255], val_loss: 1.4389, val_acc: 0.4627\n",
      "Epoch [256], val_loss: 1.4389, val_acc: 0.4627\n",
      "Epoch [257], val_loss: 1.4389, val_acc: 0.4627\n",
      "Epoch [258], val_loss: 1.4388, val_acc: 0.4627\n",
      "Epoch [259], val_loss: 1.4388, val_acc: 0.4627\n",
      "Epoch [260], val_loss: 1.4387, val_acc: 0.4627\n",
      "Epoch [261], val_loss: 1.4387, val_acc: 0.4627\n",
      "Epoch [262], val_loss: 1.4386, val_acc: 0.4632\n",
      "Epoch [263], val_loss: 1.4386, val_acc: 0.4632\n",
      "Epoch [264], val_loss: 1.4385, val_acc: 0.4632\n",
      "Epoch [265], val_loss: 1.4385, val_acc: 0.4627\n",
      "Epoch [266], val_loss: 1.4384, val_acc: 0.4632\n",
      "Epoch [267], val_loss: 1.4384, val_acc: 0.4632\n",
      "Epoch [268], val_loss: 1.4383, val_acc: 0.4632\n",
      "Epoch [269], val_loss: 1.4383, val_acc: 0.4632\n",
      "Epoch [270], val_loss: 1.4382, val_acc: 0.4632\n",
      "Epoch [271], val_loss: 1.4382, val_acc: 0.4632\n",
      "Epoch [272], val_loss: 1.4381, val_acc: 0.4632\n",
      "Epoch [273], val_loss: 1.4381, val_acc: 0.4632\n",
      "Epoch [274], val_loss: 1.4381, val_acc: 0.4632\n",
      "Epoch [275], val_loss: 1.4380, val_acc: 0.4632\n",
      "Epoch [276], val_loss: 1.4380, val_acc: 0.4632\n",
      "Epoch [277], val_loss: 1.4379, val_acc: 0.4632\n",
      "Epoch [278], val_loss: 1.4379, val_acc: 0.4632\n",
      "Epoch [279], val_loss: 1.4378, val_acc: 0.4632\n",
      "Epoch [280], val_loss: 1.4378, val_acc: 0.4632\n",
      "Epoch [281], val_loss: 1.4377, val_acc: 0.4632\n",
      "Epoch [282], val_loss: 1.4377, val_acc: 0.4632\n",
      "Epoch [283], val_loss: 1.4376, val_acc: 0.4632\n",
      "Epoch [284], val_loss: 1.4376, val_acc: 0.4632\n",
      "Epoch [285], val_loss: 1.4375, val_acc: 0.4632\n",
      "Epoch [286], val_loss: 1.4375, val_acc: 0.4632\n",
      "Epoch [287], val_loss: 1.4375, val_acc: 0.4632\n",
      "Epoch [288], val_loss: 1.4374, val_acc: 0.4632\n",
      "Epoch [289], val_loss: 1.4374, val_acc: 0.4632\n",
      "Epoch [290], val_loss: 1.4373, val_acc: 0.4632\n",
      "Epoch [291], val_loss: 1.4373, val_acc: 0.4637\n",
      "Epoch [292], val_loss: 1.4372, val_acc: 0.4637\n",
      "Epoch [293], val_loss: 1.4372, val_acc: 0.4637\n",
      "Epoch [294], val_loss: 1.4371, val_acc: 0.4637\n",
      "Epoch [295], val_loss: 1.4371, val_acc: 0.4637\n",
      "Epoch [296], val_loss: 1.4371, val_acc: 0.4637\n",
      "Epoch [297], val_loss: 1.4370, val_acc: 0.4637\n",
      "Epoch [298], val_loss: 1.4370, val_acc: 0.4637\n",
      "Epoch [299], val_loss: 1.4369, val_acc: 0.4637\n",
      "Epoch [300], val_loss: 1.4369, val_acc: 0.4637\n",
      "Epoch [301], val_loss: 1.4368, val_acc: 0.4637\n",
      "Epoch [302], val_loss: 1.4368, val_acc: 0.4637\n",
      "Epoch [303], val_loss: 1.4368, val_acc: 0.4637\n",
      "Epoch [304], val_loss: 1.4367, val_acc: 0.4637\n",
      "Epoch [305], val_loss: 1.4367, val_acc: 0.4637\n",
      "Epoch [306], val_loss: 1.4366, val_acc: 0.4642\n",
      "Epoch [307], val_loss: 1.4366, val_acc: 0.4642\n",
      "Epoch [308], val_loss: 1.4365, val_acc: 0.4642\n",
      "Epoch [309], val_loss: 1.4365, val_acc: 0.4642\n",
      "Epoch [310], val_loss: 1.4365, val_acc: 0.4642\n",
      "Epoch [311], val_loss: 1.4364, val_acc: 0.4647\n",
      "Epoch [312], val_loss: 1.4364, val_acc: 0.4647\n",
      "Epoch [313], val_loss: 1.4363, val_acc: 0.4647\n",
      "Epoch [314], val_loss: 1.4363, val_acc: 0.4647\n",
      "Epoch [315], val_loss: 1.4363, val_acc: 0.4647\n",
      "Epoch [316], val_loss: 1.4362, val_acc: 0.4647\n",
      "Epoch [317], val_loss: 1.4362, val_acc: 0.4647\n",
      "Epoch [318], val_loss: 1.4361, val_acc: 0.4647\n",
      "Epoch [319], val_loss: 1.4361, val_acc: 0.4647\n",
      "Epoch [320], val_loss: 1.4360, val_acc: 0.4647\n",
      "Epoch [321], val_loss: 1.4360, val_acc: 0.4647\n",
      "Epoch [322], val_loss: 1.4360, val_acc: 0.4647\n",
      "Epoch [323], val_loss: 1.4359, val_acc: 0.4647\n",
      "Epoch [324], val_loss: 1.4359, val_acc: 0.4647\n",
      "Epoch [325], val_loss: 1.4358, val_acc: 0.4647\n",
      "Epoch [326], val_loss: 1.4358, val_acc: 0.4647\n",
      "Epoch [327], val_loss: 1.4358, val_acc: 0.4647\n",
      "Epoch [328], val_loss: 1.4357, val_acc: 0.4647\n",
      "Epoch [329], val_loss: 1.4357, val_acc: 0.4647\n",
      "Epoch [330], val_loss: 1.4356, val_acc: 0.4651\n",
      "Epoch [331], val_loss: 1.4356, val_acc: 0.4651\n",
      "Epoch [332], val_loss: 1.4356, val_acc: 0.4651\n",
      "Epoch [333], val_loss: 1.4355, val_acc: 0.4651\n",
      "Epoch [334], val_loss: 1.4355, val_acc: 0.4651\n",
      "Epoch [335], val_loss: 1.4355, val_acc: 0.4656\n",
      "Epoch [336], val_loss: 1.4354, val_acc: 0.4651\n",
      "Epoch [337], val_loss: 1.4354, val_acc: 0.4656\n",
      "Epoch [338], val_loss: 1.4353, val_acc: 0.4656\n",
      "Epoch [339], val_loss: 1.4353, val_acc: 0.4651\n",
      "Epoch [340], val_loss: 1.4353, val_acc: 0.4651\n",
      "Epoch [341], val_loss: 1.4352, val_acc: 0.4656\n",
      "Epoch [342], val_loss: 1.4352, val_acc: 0.4656\n",
      "Epoch [343], val_loss: 1.4351, val_acc: 0.4656\n",
      "Epoch [344], val_loss: 1.4351, val_acc: 0.4661\n",
      "Epoch [345], val_loss: 1.4351, val_acc: 0.4656\n",
      "Epoch [346], val_loss: 1.4350, val_acc: 0.4656\n",
      "Epoch [347], val_loss: 1.4350, val_acc: 0.4661\n",
      "Epoch [348], val_loss: 1.4349, val_acc: 0.4661\n",
      "Epoch [349], val_loss: 1.4349, val_acc: 0.4656\n",
      "Epoch [350], val_loss: 1.4349, val_acc: 0.4661\n",
      "Epoch [351], val_loss: 1.4348, val_acc: 0.4661\n",
      "Epoch [352], val_loss: 1.4348, val_acc: 0.4661\n",
      "Epoch [353], val_loss: 1.4348, val_acc: 0.4661\n",
      "Epoch [354], val_loss: 1.4347, val_acc: 0.4661\n",
      "Epoch [355], val_loss: 1.4347, val_acc: 0.4661\n",
      "Epoch [356], val_loss: 1.4346, val_acc: 0.4661\n",
      "Epoch [357], val_loss: 1.4346, val_acc: 0.4661\n",
      "Epoch [358], val_loss: 1.4346, val_acc: 0.4661\n",
      "Epoch [359], val_loss: 1.4345, val_acc: 0.4661\n",
      "Epoch [360], val_loss: 1.4345, val_acc: 0.4661\n",
      "Epoch [361], val_loss: 1.4345, val_acc: 0.4661\n",
      "Epoch [362], val_loss: 1.4344, val_acc: 0.4661\n",
      "Epoch [363], val_loss: 1.4344, val_acc: 0.4661\n",
      "Epoch [364], val_loss: 1.4344, val_acc: 0.4661\n",
      "Epoch [365], val_loss: 1.4343, val_acc: 0.4661\n",
      "Epoch [366], val_loss: 1.4343, val_acc: 0.4661\n",
      "Epoch [367], val_loss: 1.4342, val_acc: 0.4661\n",
      "Epoch [368], val_loss: 1.4342, val_acc: 0.4661\n",
      "Epoch [369], val_loss: 1.4342, val_acc: 0.4661\n",
      "Epoch [370], val_loss: 1.4341, val_acc: 0.4661\n",
      "Epoch [371], val_loss: 1.4341, val_acc: 0.4661\n",
      "Epoch [372], val_loss: 1.4341, val_acc: 0.4661\n",
      "Epoch [373], val_loss: 1.4340, val_acc: 0.4661\n",
      "Epoch [374], val_loss: 1.4340, val_acc: 0.4661\n",
      "Epoch [375], val_loss: 1.4340, val_acc: 0.4661\n",
      "Epoch [376], val_loss: 1.4339, val_acc: 0.4661\n",
      "Epoch [377], val_loss: 1.4339, val_acc: 0.4661\n",
      "Epoch [378], val_loss: 1.4338, val_acc: 0.4661\n",
      "Epoch [379], val_loss: 1.4338, val_acc: 0.4661\n",
      "Epoch [380], val_loss: 1.4338, val_acc: 0.4666\n",
      "Epoch [381], val_loss: 1.4337, val_acc: 0.4661\n",
      "Epoch [382], val_loss: 1.4337, val_acc: 0.4661\n",
      "Epoch [383], val_loss: 1.4337, val_acc: 0.4661\n",
      "Epoch [384], val_loss: 1.4336, val_acc: 0.4666\n",
      "Epoch [385], val_loss: 1.4336, val_acc: 0.4666\n",
      "Epoch [386], val_loss: 1.4336, val_acc: 0.4666\n",
      "Epoch [387], val_loss: 1.4335, val_acc: 0.4666\n",
      "Epoch [388], val_loss: 1.4335, val_acc: 0.4666\n",
      "Epoch [389], val_loss: 1.4335, val_acc: 0.4666\n",
      "Epoch [390], val_loss: 1.4334, val_acc: 0.4666\n",
      "Epoch [391], val_loss: 1.4334, val_acc: 0.4666\n",
      "Epoch [392], val_loss: 1.4334, val_acc: 0.4666\n",
      "Epoch [393], val_loss: 1.4333, val_acc: 0.4666\n",
      "Epoch [394], val_loss: 1.4333, val_acc: 0.4666\n",
      "Epoch [395], val_loss: 1.4333, val_acc: 0.4666\n",
      "Epoch [396], val_loss: 1.4332, val_acc: 0.4666\n",
      "Epoch [397], val_loss: 1.4332, val_acc: 0.4666\n",
      "Epoch [398], val_loss: 1.4332, val_acc: 0.4666\n",
      "Epoch [399], val_loss: 1.4331, val_acc: 0.4666\n",
      "Epoch [400], val_loss: 1.4331, val_acc: 0.4666\n",
      "Epoch [401], val_loss: 1.4331, val_acc: 0.4666\n",
      "Epoch [402], val_loss: 1.4330, val_acc: 0.4666\n",
      "Epoch [403], val_loss: 1.4330, val_acc: 0.4666\n",
      "Epoch [404], val_loss: 1.4330, val_acc: 0.4666\n",
      "Epoch [405], val_loss: 1.4329, val_acc: 0.4666\n",
      "Epoch [406], val_loss: 1.4329, val_acc: 0.4666\n",
      "Epoch [407], val_loss: 1.4329, val_acc: 0.4666\n",
      "Epoch [408], val_loss: 1.4328, val_acc: 0.4666\n",
      "Epoch [409], val_loss: 1.4328, val_acc: 0.4666\n",
      "Epoch [410], val_loss: 1.4328, val_acc: 0.4666\n",
      "Epoch [411], val_loss: 1.4327, val_acc: 0.4666\n",
      "Epoch [412], val_loss: 1.4327, val_acc: 0.4666\n",
      "Epoch [413], val_loss: 1.4327, val_acc: 0.4666\n",
      "Epoch [414], val_loss: 1.4326, val_acc: 0.4666\n",
      "Epoch [415], val_loss: 1.4326, val_acc: 0.4666\n",
      "Epoch [416], val_loss: 1.4326, val_acc: 0.4666\n",
      "Epoch [417], val_loss: 1.4325, val_acc: 0.4666\n",
      "Epoch [418], val_loss: 1.4325, val_acc: 0.4666\n",
      "Epoch [419], val_loss: 1.4325, val_acc: 0.4666\n",
      "Epoch [420], val_loss: 1.4324, val_acc: 0.4666\n",
      "Epoch [421], val_loss: 1.4324, val_acc: 0.4666\n",
      "Epoch [422], val_loss: 1.4324, val_acc: 0.4666\n",
      "Epoch [423], val_loss: 1.4323, val_acc: 0.4666\n",
      "Epoch [424], val_loss: 1.4323, val_acc: 0.4666\n",
      "Epoch [425], val_loss: 1.4323, val_acc: 0.4666\n",
      "Epoch [426], val_loss: 1.4322, val_acc: 0.4666\n",
      "Epoch [427], val_loss: 1.4322, val_acc: 0.4666\n",
      "Epoch [428], val_loss: 1.4322, val_acc: 0.4666\n",
      "Epoch [429], val_loss: 1.4321, val_acc: 0.4666\n",
      "Epoch [430], val_loss: 1.4321, val_acc: 0.4666\n",
      "Epoch [431], val_loss: 1.4321, val_acc: 0.4666\n",
      "Epoch [432], val_loss: 1.4320, val_acc: 0.4666\n",
      "Epoch [433], val_loss: 1.4320, val_acc: 0.4666\n",
      "Epoch [434], val_loss: 1.4320, val_acc: 0.4666\n",
      "Epoch [435], val_loss: 1.4320, val_acc: 0.4666\n",
      "Epoch [436], val_loss: 1.4319, val_acc: 0.4666\n",
      "Epoch [437], val_loss: 1.4319, val_acc: 0.4666\n",
      "Epoch [438], val_loss: 1.4319, val_acc: 0.4666\n",
      "Epoch [439], val_loss: 1.4318, val_acc: 0.4666\n",
      "Epoch [440], val_loss: 1.4318, val_acc: 0.4666\n",
      "Epoch [441], val_loss: 1.4318, val_acc: 0.4666\n",
      "Epoch [442], val_loss: 1.4317, val_acc: 0.4666\n",
      "Epoch [443], val_loss: 1.4317, val_acc: 0.4666\n",
      "Epoch [444], val_loss: 1.4317, val_acc: 0.4666\n",
      "Epoch [445], val_loss: 1.4316, val_acc: 0.4666\n",
      "Epoch [446], val_loss: 1.4316, val_acc: 0.4671\n",
      "Epoch [447], val_loss: 1.4316, val_acc: 0.4671\n",
      "Epoch [448], val_loss: 1.4316, val_acc: 0.4671\n",
      "Epoch [449], val_loss: 1.4315, val_acc: 0.4671\n",
      "Epoch [450], val_loss: 1.4315, val_acc: 0.4671\n",
      "Epoch [451], val_loss: 1.4315, val_acc: 0.4666\n",
      "Epoch [452], val_loss: 1.4314, val_acc: 0.4671\n",
      "Epoch [453], val_loss: 1.4314, val_acc: 0.4666\n",
      "Epoch [454], val_loss: 1.4314, val_acc: 0.4666\n",
      "Epoch [455], val_loss: 1.4313, val_acc: 0.4666\n",
      "Epoch [456], val_loss: 1.4313, val_acc: 0.4666\n",
      "Epoch [457], val_loss: 1.4313, val_acc: 0.4666\n",
      "Epoch [458], val_loss: 1.4312, val_acc: 0.4666\n",
      "Epoch [459], val_loss: 1.4312, val_acc: 0.4666\n",
      "Epoch [460], val_loss: 1.4312, val_acc: 0.4666\n",
      "Epoch [461], val_loss: 1.4312, val_acc: 0.4666\n",
      "Epoch [462], val_loss: 1.4311, val_acc: 0.4666\n",
      "Epoch [463], val_loss: 1.4311, val_acc: 0.4666\n",
      "Epoch [464], val_loss: 1.4311, val_acc: 0.4666\n",
      "Epoch [465], val_loss: 1.4310, val_acc: 0.4666\n",
      "Epoch [466], val_loss: 1.4310, val_acc: 0.4666\n",
      "Epoch [467], val_loss: 1.4310, val_acc: 0.4666\n",
      "Epoch [468], val_loss: 1.4310, val_acc: 0.4666\n",
      "Epoch [469], val_loss: 1.4309, val_acc: 0.4666\n",
      "Epoch [470], val_loss: 1.4309, val_acc: 0.4666\n",
      "Epoch [471], val_loss: 1.4309, val_acc: 0.4666\n",
      "Epoch [472], val_loss: 1.4308, val_acc: 0.4666\n",
      "Epoch [473], val_loss: 1.4308, val_acc: 0.4666\n",
      "Epoch [474], val_loss: 1.4308, val_acc: 0.4671\n",
      "Epoch [475], val_loss: 1.4307, val_acc: 0.4671\n",
      "Epoch [476], val_loss: 1.4307, val_acc: 0.4671\n",
      "Epoch [477], val_loss: 1.4307, val_acc: 0.4671\n",
      "Epoch [478], val_loss: 1.4307, val_acc: 0.4671\n",
      "Epoch [479], val_loss: 1.4306, val_acc: 0.4671\n",
      "Epoch [480], val_loss: 1.4306, val_acc: 0.4671\n",
      "Epoch [481], val_loss: 1.4306, val_acc: 0.4671\n",
      "Epoch [482], val_loss: 1.4305, val_acc: 0.4671\n",
      "Epoch [483], val_loss: 1.4305, val_acc: 0.4671\n",
      "Epoch [484], val_loss: 1.4305, val_acc: 0.4671\n",
      "Epoch [485], val_loss: 1.4305, val_acc: 0.4671\n",
      "Epoch [486], val_loss: 1.4304, val_acc: 0.4671\n",
      "Epoch [487], val_loss: 1.4304, val_acc: 0.4671\n",
      "Epoch [488], val_loss: 1.4304, val_acc: 0.4671\n",
      "Epoch [489], val_loss: 1.4303, val_acc: 0.4671\n",
      "Epoch [490], val_loss: 1.4303, val_acc: 0.4671\n",
      "Epoch [491], val_loss: 1.4303, val_acc: 0.4671\n",
      "Epoch [492], val_loss: 1.4303, val_acc: 0.4671\n",
      "Epoch [493], val_loss: 1.4302, val_acc: 0.4671\n",
      "Epoch [494], val_loss: 1.4302, val_acc: 0.4671\n",
      "Epoch [495], val_loss: 1.4302, val_acc: 0.4671\n",
      "Epoch [496], val_loss: 1.4301, val_acc: 0.4671\n",
      "Epoch [497], val_loss: 1.4301, val_acc: 0.4671\n",
      "Epoch [498], val_loss: 1.4301, val_acc: 0.4671\n",
      "Epoch [499], val_loss: 1.4301, val_acc: 0.4671\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[348], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history3 \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[343], line 11\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, lr, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m opt_func(model\u001b[38;5;241m.\u001b[39mparameters(), lr)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m## Training Phas\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     12\u001b[0m         loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining_step(batch)\n\u001b[1;32m     13\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[331], line 26\u001b[0m, in \u001b[0;36mCustomDatasetFromDepthImages.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     24\u001b[0m single_image_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_arr[index]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Open image\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m img_as_img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_image_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m img_as_img \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000.0\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Transform image to tensor\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/format.py:809\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m    822\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history3 = fit(1000, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "5719562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [1], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [2], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [3], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [4], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [5], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [6], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [7], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [8], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [9], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [10], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [11], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [12], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [13], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [14], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [15], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [16], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [17], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [18], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [19], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [20], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [21], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [22], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [23], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [24], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [25], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [26], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [27], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [28], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [29], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [30], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [31], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [32], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [33], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [34], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [35], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [36], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [37], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [38], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [39], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [40], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [41], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [42], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [43], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [44], val_loss: 1.3687, val_acc: 0.5629\n",
      "Epoch [45], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [46], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [47], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [48], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [49], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [50], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [51], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [52], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [53], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [54], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [55], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [56], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [57], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [58], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [59], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [60], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [61], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [62], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [63], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [64], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [65], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [66], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [67], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [68], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [69], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [70], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [71], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [72], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [73], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [74], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [75], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [76], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [77], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [78], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [79], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [80], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [81], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [82], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [83], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [84], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [85], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [86], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [87], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [88], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [89], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [90], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [91], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [92], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [93], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [94], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [95], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [96], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [97], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [98], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [99], val_loss: 1.3686, val_acc: 0.5629\n",
      "Epoch [100], val_loss: 1.3686, val_acc: 0.5629\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[327], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history4 \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[322], line 12\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, lr, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m## Training Phas\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 12\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[319], line 24\u001b[0m, in \u001b[0;36mMnistModel.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m     23\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 24\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m## Generate predictions\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out, labels) \u001b[38;5;66;03m## Calculate the loss\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[319], line 15\u001b[0m, in \u001b[0;36mMnistModel.forward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xb):\n\u001b[0;32m---> 15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(out)\n\u001b[1;32m     17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history4 = fit(10000, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113113c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history5 = fit(100000, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "deb8c5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy Vs. No. of epochs')"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Replace these values with your result\n",
    "history = [result0] + history1 #+ history2# + history3 + history4\n",
    "accuracies = [result['val_acc'] for result in history]\n",
    "plt.plot(accuracies)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy Vs. No. of epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "98761a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/alex/Documents/NN_path_planning/nn/model/model_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "c2a8da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = torch.load('/home/alex/Documents/NN_path_planning/nn/model/model_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "5173a1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10005/1904010061.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  action = F.softmax(output).detach().numpy().argmax()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model_1.forward(custom_dataset[3230][0])\n",
    "# _, pred = torch.max(output)\n",
    "action = F.softmax(output).detach().numpy().argmax()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8230df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67286ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
